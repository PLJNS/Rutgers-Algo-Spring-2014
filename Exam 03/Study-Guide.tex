\documentclass[a4paper,11pt]{article}

\usepackage{lmodern}

\hyphenation{Bayes-ian}
\hyphenation{Bayes-ian-ism}

\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{snakes}
\usetikzlibrary{
  decorations,
  decorations.markings,
  hobby,
}

\usepackage{endnotes}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{ifxetex}
\usepackage{ifluatex}
\usepackage{fixltx2e} 

\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}


\else
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}



\fi

\IfFileExists{microtype.sty}{\usepackage{microtype}}{}

\usepackage{geometry}
\geometry{letterpaper,tmargin=0.5in,bmargin=0.5in,lmargin=0.5in,rmargin=0.5in,headheight=0in,headsep=0in,footskip=.25in}

\usepackage{natbib}
\bibpunct{(}{)}{,}{a}{}{,}

\bibliography{biblio.bib}





\usepackage{longtable,booktabs}


\ifxetex
  \usepackage[setpagesize=false,
              unicode=false,
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi

\urlstyle{same}



\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}



\renewcommand\thefootnote{\textcolor{black}{\arabic{footnote}}}


\newtheorem{proposition}{\textbf{Proposition}}[section]
\newtheorem{theorem}[proposition]{\textbf{Theorem}}
\newtheorem{claim}[proposition]{\textbf{Claim}}
\newtheorem{lemma}[proposition]{\textbf{Lemma}}
\newtheorem{definition}[proposition]{\textbf{Definition}}
\newtheorem{example}[proposition]{\textbf{Example}}
\newtheorem{corollary}[proposition]{\textbf{Corollary}}
\newtheorem{principle}[proposition]{\textbf{Principle}}
\newtheorem{remark}[proposition]{\textbf{Remark}}
\newtheorem{algorithm}[proposition]{\textbf{Algorithm}}
\newtheorem{observation}[proposition]{\textbf{Observation}}



\title{Study Guide}
\author{Paul Jones \\ {\small  Design and Analysis of Computer Algorithms} \\ {\small Professor Kostas Bekris} \\ {\small Rutgers University}}

\usepackage{setspace}

\begin{document}

\doublespacing

\maketitle


\singlespacing

%\begin{abstract}
%\end{abstract}

{
\clearpage
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{3}
\tableofcontents
\clearpage
}



\section{Midterm 1 Study Guide}\label{midterm-1-study-guide}

\subsection{Introduction to Concepts of Algorithmic Design, Computing
Running
Times}\label{introduction-to-concepts-of-algorithmic-design-computing-running-times}

\subsubsection{Practice questions}\label{practice-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{You may be provided with an algorithm for computing the $n$th
  Fibonacci number and then be asked to compute its running time (think
  in terms of it complexity). Alternatively, you may be asked to provide
  an efficient algorithm for computing Fibonacci numbers.}

\begin{verbatim}
function fib1(n) 
if n = 0: return 0
if n = 1: return 1
return fib1(n - 1) + fib1(n - 2)
\end{verbatim}

\begin{verbatim}
(n) function fib2
if n=0 return 0
create an array f[0 . . . n] f[0] =0,f[1] =1
for i = 2 . . . n:
    f[i] = f[i - 1] + f[i - 2] 
return f[n]
\end{verbatim}
\item
  \textbf{Give a justification why improvement in hardware are typically
  not sufficient to make an algorithm with exponential running time
  reasonably tractable.}

  \begin{quote}
  But technology is rapidly improving---computer speeds have been
  doubling roughly every 18 months, a phenomenon sometimes called
  Moore's law. With this extraordinary growth, perhaps fib1 will run a
  lot faster on next year's machines. Let's see---the running time of
  fib1(n) is proportional to $2^{0.694n} \approx (1.6)^n$, so it takes
  1.6 times longer to compute $F_{n+1} than F_n$. And under Moore's law,
  computers get roughly 1.6 times faster each year. So if we can
  reasonably compute $F_{100}$ with this year's technology, then next
  year we will manage $F_{101}$. And the year after, $F_{102}$. And so
  on: just one more Fibonacci number every year! Such is the curse of
  exponential time.
  \end{quote}
\item
  \textbf{We have 3 algorithms for the same problem where the first runs
  in exponential time, the second in logarithmic, and the thir in linear
  time as a function of the same input. Which algorith do you prefer to
  use?}

  \begin{longtable}[c]{@{}ll@{}}
  \toprule\addlinespace
  Name & Running time
  \\\addlinespace
  \midrule\endhead
  Exponential & $O(a^n)$
  \\\addlinespace
  Logarithmic & $(\log(n))$
  \\\addlinespace
  \textbf{Linear} & $O(n)$
  \\\addlinespace
  \bottomrule
  \end{longtable}
\item
  \textbf{You may be provided running times of different algorithms and
  asked to show which running time dominates asymptotically.}
\item
  \textbf{Provide the definition of \emph{O}-notation (or Theta or
  Omega).}

  \begin{quote}
  Let $f(n)$ and $g(n)$ be functions from positive integers to positive
  reals. We say f = O(g) (which means that ``$f$ grows no faster than
  $g$'') if there is a constant $c > 0$ such that
  $f(n) \le c \times g(n)$.
  \end{quote}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Just as $O(\cdot)$ is an analog of $\le$, you can define the other
    analyses as such:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      $f = \Omega(g)$ means $g = O(f)$.
    \item
      $f = \Theta(g)$ means $f = O(g)$ and $f = \Omega(g)$.
    \end{itemize}
  \end{itemize}
\item
  **You may be provided certain statements about asymptotic analysis of
  running times and asked to show if they are correct or not. For
  instance, ``n to the a dominates n to the b if a is greater than b.**
\end{enumerate}

\subsection{Number Theoretic Algorithms Complexity of adding and
multiplying 2 n-bit numbers, modulo
arithmetic}\label{number-theoretic-algorithms-complexity-of-adding-and-multiplying-2-n-bit-numbers-modulo-arithmetic}

\subsubsection{Chapter 31.2 from CLRS2: Greatest common
divisor}\label{chapter-31.2-from-clrs2-greatest-common-divisor}

\begin{itemize}
\item
  In this section, Euclid's algorithm for computing the GCD of two
  integers efficiently.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Suprising connection with Fibonacci numbers.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      This yields the worst case.
    \end{itemize}
  \end{itemize}
\item
  Restricted to nonegative integers. \[\gcd(a, b) = \gcd(|a|, |b|)\]
\item
  One way of characterizing the problem:

  \[a = p_{1}^{e_1}p_{2}^{e_2} \cdots p_{r}^{e_r}\]
  \[b = p_{1}^{f_1}p_{2}^{f_2} \cdots p_{r}^{f_r}\]
  \[\gcd(a, b) = p_{1}^{\min(e_1, f_1)} p_{2}^{\min(e_2, f_2)} \cdots p_{r}^{\min(e_r, f_r)}\]
\item
  Proof

  \begin{itemize}
  \item
    We first show that the GCD of $a$ and $b$ divide each other.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      If $d = \gcd(a, b)$, then $d | a$ and $d | b$.
    \end{itemize}
  \item
    Because $a \bmod b$ is a linear combination of $a$ and $b$,
    $d | (a \bmod b)$.
  \item
    Because $d | b$ and $d | (a \bmod b)$, $d | \gcd(b, a \bmod b)$
  \item
    Equivilently,

    \[\gcd(a, b) | \gcd(b, a \bmod b)\]
  \item
    You can show that the reversed is true to, and because if you can
    invert the divisible operator, the two things are plus or minus
    equal
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the ``primality'' problem and what is the
  ``factoring'' problem? Which one of the two is tractable? What are the
  advantages of the intractability of the other problem?}

  \begin{description}
  \item[Factoring problem]
  Given a number $N$, express it as a product of prime factors.

  Computationally intractable, fastest is exponential. This makes RSA
  work because keys and factoring large multiples.
  \item[Primality problem]
  Given a number $N$, determine whether it is a prime.

  Computationally tractable.
  \end{description}
\item
  \textbf{How many digits do you need to represent a number \emph{N} in
  base \emph{b}?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    With $k$ digits in base $b$ we can express numbers up to $b^k - 1$.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      For instance, in decimal, three digits get us all the way up to
      $999 = 10^3 - 1$.
    \end{itemize}
  \item
    By solving for $k$, we find that $\lceil \log_b(N + 1) \rceil$ are
    need to write $N$ in base $b$.
  \end{itemize}
\item
  \textbf{How much does the size of the representation of a number
  changes when we change bases?}

  \begin{itemize}
  \item
    Recall the rule for converting logarithms from base $a$ to base $b$:

    \[\log_b N = (log_a N) / (log_a b)\]

    \begin{itemize}
    \item
      So the size of integer $N$ in base $a$ is the same as its size in
      base $b$, times a constant factor $log_a b$.
    \item
      In big-O, the base is irrelevant, and we write the size simply as

      \[O(\log N)\]
    \end{itemize}
  \end{itemize}
\item
  \textbf{What is the running time of the addition operation of a number
  for two \emph{n}-bit numbers (think in terms of bit complexity)?}

  \begin{itemize}
  \item
    Suppose $x$ and $y$ are $n$ bits long.
  \item
    The sume of $x$ and $y$ is \emph{at most} $n + 1$ bits long.
  \item
    Each bit of the sume is computed in a fixed amount of time.
  \item
    The total running time for addition will be

    \[c_0 + c_1 n\]

    In other words, \emph{linear}.

    \[O(n)\]
  \end{itemize}
\item
  \textbf{What is the running time of the tradition multiplication
  operation taught in grade school for two \emph{n}-bit numbers (think
  in terms of \emph{n}-bit complexit)?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    To compute each row, either ``X'' or ``0'', left-shifted
  \item
    The rows are in the order of $2n$.
  \item
    You have to sum them up, and you do this pairwise.
  \item
    If you do $n$ times an operation which costs $n$, your running time
    is going to be $n^2$.
  \end{itemize}
\item
  \textbf{Provide a recursive formula for the multiplication of two
  numbers.}

\begin{verbatim}
function multiplication(x, y) {
    if (y == 1) {
        return x;
    }

    else {
        return x + multiplication(x, y - 1);
    }
}
\end{verbatim}
\item
  \textbf{What is the complexity of modular addition and modular
  multiplication for two \emph{n} bit numbers?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \textbf{Addition}: $O(n)$
  \item
    \textbf{Multiplication}: $O(n^2)$
  \end{itemize}
\end{enumerate}

\subsection{RSA Cryptosystem, Fermat's Little Theorem and Modular
Exponentiation}\label{rsa-cryptosystem-fermats-little-theorem-and-modular-exponentiation}

\subsubsection{Practice questions}\label{practice-questions-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Give a private key protocol for a cryptography application.
  Given an example it can be compromised.}

  \begin{itemize}
  \item
    There is the classic ``codebook'' private-key scheme, where Alice
    and Bob meet before hand and agree on a secret code.
  \item
    The way that this is compromised is if Eve gets the codebook or
    knows a given message and backtraces the code from it.
  \item
    Alternatively, the answer you're looking for might be that you can
    encode and decode with public keys like:

    \[x = d(e(x))\]

    Where $x$ is a message, $d(\cdot)$ is a decoder, and $e(\cdot)$ is
    an encoder. Bob can
  \item
    ``Network sniffer''
  \end{itemize}
\item
  \textbf{RSA is based on which basic property of modulo arithmetic?}

  \begin{itemize}
  \item
    The basic feature of modulo arithmetic that RSA exploits is the
    dramatic contrast in the tractibility of factoring and primality
    testing.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Bob and Alice only need to make simple calculations.
    \item
      Eve needs to make computations that would be struggle for the
      world's largest computers.
    \end{itemize}
  \item
    Pick any two prime $p$ and $q$ and let $N = pq$. For any $e$
    relatively prime to $(p - 1)(q - 1)$:

    \[(x^e)^d \equiv x \mod N\]
  \end{itemize}
\item
  \textbf{Describe the steps of the RSA protocol. The security of the
  protocol is based on which assumption?}

  \begin{quote}
  Given $N$, $e$, and $y = x^e \mod N$, it is computationally
  intractable to determine $x$.
  \end{quote}
\item
  \textbf{What are the basic operations that need to be performed
  accords to the RSA protocol and what is their running time?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Pick two large primes
  \item
    Multiply two large primes
  \item
    Extended Euclid algorithm
  \item
    Efficient modular exponentiation algorithm
  \item
    $y^d \mod N$
  \end{itemize}
\item
  \textbf{You may be provided an example message and asked to described
  the operations of the RSA protocol on it.}
\item
  \textbf{What does Fermat's Little Theorem specify? Prove it.}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \textbf{Fermat's Little Theorem}: If $p$ is a prime number, then for
    any integer $a$, the number $a^p - a$ is an integer multiple of $p$.
  \item
    \textbf{Proof}

    \begin{itemize}
    \item
      Let us assume that $a$ is positive and not divisible by $p$. The
      idea is that if we write down the sequence of numbers

      \[a 2a, 3a, \cdots, (p - 1)a\]

      and reduce each one modulo $p$, the result sequences turns out to
      be an arrangment of

      \[1, 2, 3, \cdots, p - 1\]

      Therefore, if we multiple together the numbers in each sequences,
      the results must be indetifical modulo $p$:

      \[a \times 2a \times 3a \times \cdots \times (p - 1)\]

      Which is equivilent to

      \[1 \times 2 \times 3 \times \cdots \times (p - 1)\]

      Collecting the $a$ terms yields

      \[a^{p - 1} (p - 1)! \equiv (p - 1)! (\mod p)\]

      Finally, we may ``cancel out'' the numbers $1, 2, \cdots, p - 1$
      from both sides, obtains,

      \[a^{p -1} \equiv 1 (\mod p)\]
    \end{itemize}
  \end{itemize}
\item
  \textbf{What is the importance of Fermat's little theorem in the RSA
  protocol?}
\item
  \textbf{How can we efficiently perform modular exponentiantion? What
  is the running time of the approach?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Recursion, repeated squaring.
  \item
    Let $n$ be the size of bits $x$, $y$, and $N$ (whichever is
    largest).
  \item
    Like multiplication, there are \emph{at most} $n$ recursive calls.
  \item
    During each call, it multiples $n$-bit numbers.
  \item
    Doing computation modulo $N$ saves us here.
  \item
    Running time of $O(n^3)$
  \end{itemize}
\end{enumerate}

\subsection{Greatest Common Divisor algorithms: Euclid's test, Modulo
Multiplicative
Inverse}\label{greatest-common-divisor-algorithms-euclids-test-modulo-multiplicative-inverse}

\subsubsection{Practice questions}\label{practice-questions-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What does Euclid's rule specify? Prove it.}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \textbf{Euclid's rule}: If $x$ and $y$ are positive integers where
    $x \le y$, then $\gcd(x, y) = \gcd(x \bmod y, y)$
  \item
    \textbf{Proof}

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Suppose $a, b \in \mathbb{Z}$ and $a \lor B \neq 0$
    \item
      From the *Division Theorem$, $a = qb + r\$ where $0 \le r < |b|$.
    \item
      From \emph{GCD with Remainder}, the GCD of $a$ and $b$ is also the
      GCD of $b$ and $r$.
    \item
      Therefore, we may search instead for $\gcd(b, r)$.
    \item
      Since $|r| < |b|$ and $b \in \mathbb{Z}$, we will reach $r = 0$
      after finitely many steps.
    \item
      At this point, $\gcd(r, 0) = r$ from \emph{GCD with Zero}.
    \end{itemize}
  \item
    \textbf{Proof \emph{from the book}}:

    \begin{itemize}
    \item
      It is enough to show the slightly simpler rule:

      \[\gcd(x, y)= \gcd(x - y, y)\]

      from which the one stated can be derived by repeatedly subtracting
      $y$ from $x$.
    \item
      Any integer that divides both $x$ and $y$ must also divide
      $x - y$, so

      \[\gcd(x, y) \le \gcd(x - y, y)\]
    \item
      Likewise, any integer that divides both $x - y$ and $y$ must also
      divide both $x$ and $y$, so

      \[\gcd(x, y) \ge gcd(x - y, y)\]
    \end{itemize}
  \end{itemize}
\item
  \textbf{What is Euclid's algorithm for finding the greatest common
  divisor? What is its running time and why?}
  \textsubscript{\textasciitilde{}} function euclid(a, b) \{ if (b == 0)
  \{ return a; \}

\begin{verbatim}
else {
    return euclid(b, a % b);
}
\end{verbatim}

  \} \textsubscript{\textasciitilde{}}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    It is, ``If $d$ divides both $a$ and $b$, and $d = ax + by$ for some
    integers $x$ and $y$, then necessarily $d = \gcd(a, b)$.''
  \item
    \textbf{Run time}

    \begin{itemize}
    \item
      Let $T(a,b)$ be the number of steps taken in the Euclidean
      algorithm.
    \item
      Let $h = \log_{10} b$ be the number of digits in $b$.
    \item
      Assume that the modulo function is $O(1)$.
    \item
      The worst case is consecutive Fibonacci numbers.

      \[a = F_{n + 1}, b = F_{n}\]
    \item
      The algorith will calcuate the following until $n = 0$,

      \[\gcd(F_{n + 1}, F_n) = \gcd(F_n, F_{n - 1})\]

      So,

      \[T(F_{n + 1}, F_n) = \Theta(n)\] \[T(a, F_n) = O(n)\]
    \item
      Since $F_n = \Omega(\Phi^n)$, this implies that

      \[T(a, b) = O(\log_{\Phi} b)\]
    \item
      Note that $h \approx log_{10} b$ and
      $log_b x = \frac{\log x}{\log b}$, this implies
      $\log_b x = O(\log x)$.
    \item
      So the worst case for Euclid's algorith is

      \[O(\log_{\Phi}b) = O(h) = \log(b)\]
    \end{itemize}
  \end{itemize}
\item
  \textbf{Show that if $d$ divides both $a$ and $b$, and
  $d = a\times x+b \times y$ for some integers $x$ and $y$, then
  $d = \gcd(a, b)$}.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    By the first two condition, $d$ is a common divisor of $a$ and $b$
    so it cannot exceed the GCD, that is, $d \le \gcd(a, b)$.
  \item
    On the other hand, $\gcd(a, b)$ is a common divisor of $a$ and $b$,
    it must also divide $ax + by = d$, which implies that
    $\gcd(a, b) \le d$.
  \item
    Therefore, $d = \gcd(a, b)$.
  \end{itemize}
\item
  \textbf{What is the extended Euclid's algorithm for finding the
  greatest common divisor d of two numbers a and b, as well as numbers
  $x$ and $y$, so that $d = a\times x+b \times y$? Prove its correctness
  (you will need to prove Euclid's algorithm first and then the
  extension).}

  \begin{quote}
  \textbf{EEA}: If $d$ divides both $a$ and $b$, and $d = ax + by$ for
  some integers $x$ and $y$, then $d = \gcd(a, b)$
  \end{quote}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \emph{Proof}

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      By the first two condition, $d$ is a common divisor of $a$ and $b$
      so it cannot exceed the GCD, that is, $d \le \gcd(a, b)$.
    \item
      On the other hand, $\gcd(a, b)$ is a common divisor of $a$ and
      $b$, it must also divide $ax + by = d$, which implies that
      $\gcd(a, b) \le d$.
    \item
      Therefore, $d = \gcd(a, b)$.
    \end{itemize}
  \end{itemize}
\item
  \textbf{When does the multiplicative inverse of a number $x$ exists
  modulo $N$ and why?}
\item
  \textbf{How can you compute the multiplicative inverse modulo $N$ for
  two relative prime numbers? What is the running time of the
  corresponding algorithm?}
\end{enumerate}

\subsection{Primality Testing and Universal
Hashing}\label{primality-testing-and-universal-hashing}

\subsubsection{Reading material}\label{reading-material}

\paragraph{Chapter 1.3 from DPV: Primality
testing}\label{chapter-1.3-from-dpv-primality-testing}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Is there a test to tell us whether a number is prime? We place our
  home in a theorom from 1640.
\end{itemize}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Fermat's Little Theorem]
If $p$ is prime, then $\forall 1 \le a < p$,
\[a^{p - 1} \equiv 1 (\bmod p)\]
\end{description}

\subsubsection{Practice questions}\label{practice-questions-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Describe a test for evaluating whether a number is prime. What
  is the probability of this test returning the correct answer and why?
  Can you increase the probability of success for this test?}

  \begin{itemize}
  \item
    Fermat's little theorem states that if $p$ is prime and
    $1 \le a < p$, then, \[a^{p - 1} \equiv 1 (\bmod p)\]
  \item
    If we want to test if $p$ is prime, then we can pick random $a$'s in
    the interval and see if the equality holds.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      If the equlaity \emph{doesn't} hold for a value, then $p$ is
      composite.
    \item
      If the equality \emph{does} hold for \emph{many} values of $a$,
      the $p$ is \emph{probable prime}.
    \end{itemize}
  \item
    Using fast algorithms for modular exponentiation, the running time
    of this algorith is

    \[O(k \times \log^2 n \times \log \log n \times \log \log \log n)\]

    Where $k$ is the number of time we test a random $a$, and $n$ is the
    value we want to test for primality.
  \item
    You increase the probability with a larger $k$.
  \end{itemize}
\end{enumerate}

\begin{itemize}
\item
  \textbf{What does Lagrange's Prime Number Theorem specify and why is
  it helpful for primality testing?}

  \begin{description}
  \item[Lagrange's prime number theorem]
  Let $\pi(x)$ be the number of prime $\le x$. Then
  $\pi(x) \equiv x / (\ln x)$, or more precisely,

  \[\lim_{x \to \infty} \frac{\pi(x)}{(x / \ln x)} = 1\]
  \end{description}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Such abundance makes it simple to generate a random $n$-bit prime:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Pcik a random $n$-bit number $N$.
    \item
      Run a primality test on $N$.
    \item
      If it passes the test, output $N$; else repeat the process.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What properties should a good hash function provide?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Wikipedia

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Determinism, same input same output
    \item
      Uniformity, map the expected inputs as evenly as possible.
    \item
      Variable range, can be expanded or contracted in size.
    \item
      Continuity, two similar inputs should be mapped to nearly equal
      hashes
    \end{itemize}
  \item
    CLRS2

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      A good hash function satisifes the assumption of simple uniform
      hasing: each key is equally likely to hash to any of the $m$
      slots, indepedantly of where any other key has hashed to.
    \end{itemize}
  \end{itemize}
\item
  \textbf{What is the property of universal hashing families of
  functions? Provide a universal hashing family of functions for an
  example problem and prove the corresponding property.}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Universal hashing is designed to stop the effectiveness of a
    attacker who knows the hash function and wants to put every input in
    the same bin.
  \item
    To avoid this, at the beginning of the lifecycle of a hash table, a
    function is picked \emph{at random}.
  \end{itemize}

  \begin{description}
  \item[Property]
  Consider any pair of distinct IP addresses $x = (x_1, \cdots, x_4)$
  and $y = (y_1, \cdots, y_4)$. If the coefficients
  $a = (a_1, a_2, a_3, a_4)$ are chosen uniformly at random from
  $\lbrace 0, 1, \cdots, n - 1$ then

  \[Pr \lbrace h_a (x_1, \cdots, x_2) = h_a (y_1, \cdots, y_4) \rbrace = \frac{1}{n}\]
  \end{description}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \emph{Proof}

    \begin{itemize}
    \item
      Since $x = (x_1, \cdots x_4)$ and $y = (y_1, \cdots, y_4)$ are
      distinct, these quadruples must differ in some component, assume
      their not equal.
    \item
      We wish to compute the probability
      $Pr[h_a(x_1, \cdots, x_4) = h_a (y_1 \cdots y_4)]$
    \item
      That is, that
      \[ \sum_{i = 1}^{4} a_i \times x_i \equiv \sum_{i = 1}^4 a_i \times y_i \bmod n \]
    \item
      This last equation can be rewritten
      \[\sum_{i = 1}^{3} a_i \times (x_i - y_i) \equiv a_4 \times (y_4 - x_4) \bmod n \]
    \item
      Suppose that we draw a random hash function $h_a$ by picking
      $a = (a_1, a_2, a_3, a_4)$ at random.
    \item
      We start by drawing $a_1, a_2, a_3$, and then pause to think: What
      is the probability that the last drawn number $a_4$ is such that
      the equation 2 bullet points back holds (call it (1))?
    \item
      So far the left-hand side of equation (1) evaluates to some
      number, say $c$.
    \item
      And since $n$ is prime and $x_4 \neq y_4$, $(y_4 - x_4)$ has a
      unique inverse modulo $n$.
    \item
      Thus for equation (1) to hold, the last number $a_4$ must be
      precisely $c \times (y_4 - x_4)^{-1} \bmod n$, out of its $n$
      possible values.
    \item
      The probability of this happening is $\frac{1}{n}$, and the proof
      is complete.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\subsection{Divide and Conquer Algorithms and Recurrence Functions,
Mergesort}\label{divide-and-conquer-algorithms-and-recurrence-functions-mergesort}

\subsubsection{Practice questions}\label{practice-questions-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What are the basic principles of divide-and-conquer
  algorithms?}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Breaking it into \emph{subproblems} that are themselves smaller
    instancs of the same type of problem.
  \item
    Recursively solving these subproblems.
  \item
    Appropriately combining their answers.
  \end{enumerate}
\item
  \textbf{Describe an approach for multiplication of two n-bit numbers
  that has a better running time than $O(n^2)$. Prove its running time.}

\begin{verbatim}
function multiply(x, y) {
    n = max(sizeof(x), sizeof(y));

    if (n == 1) {
        return xy;
    }

    x_l = leftmostBits(ceil(n / 2));
    x_r = rightmostBits(floor(n / 2));
    y_l = leftmost(ceil(n / 2));
    y_r = rightmostBits(floor(n / 2));

    p_1 = multiply(x_l, y_l);
    p_2 = multiplu(x_r, y_r);
    p_3 = multiply(x_l + x_r, y_l + y_r);

    return p_1 x 2^n + (p_3 - p_1 - p_2) x 2^(n / 2) + p_2;
}
\end{verbatim}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Run time

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      This running time can be derived by looking at the algorithm's
      pattern of recursive calls, which form a tree-structure.
    \item
      At aech successive level of recursion the subproblems get halved
      in size. At the $log_2 n$th level, the subproblems get down to
      size 1, and so the recursion ends.
    \item
      Therefore, the high of the tree is $\log_2 n$.
    \item
      The branching factor is 4, each problem recursive produces three
      smaller ones - with the result that at depth $k$ in the tree there
      are $3^k$ subproblems, each of size $n / 2^k$.
    \end{itemize}
  \end{itemize}
\item
  \textbf{What does the Master theorem specify? Prove it.}
  \textgreater{} \textbf{Master theorem}: If
  $T(n) = aT(\lceil n / b \rceil) + O(n^d)$ for some \textgreater{}
  constants $a > 0, b > 1, d \ge 0$, then: \textgreater{}
  \[\begin{cases} O(n^d) &\mbox{if } d > \log_b a \\ O(n^d \log n) &\mbox{if } d = \log_b a \\ O(n^{\log_b a}) &\mbox{if } d < lob_b a \\ \end{cases}\]

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Proof

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Assume that $n$ is a power of $b$, for convinience rounding.
    \item
      Notice that the size of the subproblems decreases by a factor of
      $b$ with each level of recursion, and therefore reaches the base
      case after $log_b n$ levels. This is the hight of the tree.
    \item
      Its branching factor is $a$, so the $k$th level of the tree is
      made up of $a^k$ subproblems, each of size $n / b^k$. The total
      work is done:
      \[a^k \times O(\frac{n}{b^k})^d = O(n^d) \times (\frac{a}{b^d})^k\]
    \item
      As $k$ goes from 0 (the root) to $\log_b n$ (the leaves), these
      numbers form a geometric series with the ration $a / b^d$. Finding
      the sum of such a series in big-O notation is easy and comes down
      to three cases:

      \begin{enumerate}
      \def\labelenumiii{\arabic{enumiii}.}
      \itemsep1pt\parskip0pt\parsep0pt
      \item
        The ratio is less than 1.
      \item
        The ratio is greater than 1.
      \item
        The ratio is exactly 1.
      \end{enumerate}
    \end{enumerate}
  \end{itemize}
\item
  \textbf{You may be provided a divide-and-conquer algorithm and asked
  to argue about its running time by using the Master theorem.}
\item
  \textbf{How does mergesort work, what is its running time and why? How
  does the iterative version of mergesort work?}

  Mergesort splits an unsorted array into two and sorts and concatenates
  the subarrays.

\begin{verbatim}
function interative-mergesort(a[1 .... n]) {
    Q = [] (empty queue);
    for i = 1 to n:
         inject(Q, a[i])l

    while(count(Q) > 1):
         inject(Q, merge(eject(Q), eject(Q));

    return eject(Q);
}
\end{verbatim}
\end{enumerate}

\subsection{Quicksort, Lower bounds for comparison-based
sorting}\label{quicksort-lower-bounds-for-comparison-based-sorting}

\subsubsection{Practice questions}\label{practice-questions-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What are comparison sorting algorithms? What is the lower
  limit for the running time of comparison sorting algorithms?}
\item
  \textbf{How does quick-sort work? When does the worst-case running
  time arise? When does the best-case running time arise?}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Pick an element, called a pivot, from the list.
  \item
    Reorder the list so that all elements with values less than the
    pivot come before the pivot, while all elements with values greater
    than the pivot come after it (equal values can go either way). After
    this partitioning, the pivot is in its final position. This is
    called the partition operation.
  \item
    Recursively apply the above steps to the sub-list of elements with
    smaller values and separately the sub-list of elements with greater
    values.
  \end{enumerate}

  \begin{quote}
  The best case for the algorithm now occurs when all elements are
  equal. The worst case for the algorithm occurs when the elements are
  already sorted.
  \end{quote}
\item
  \textbf{Provide a rigorous proof for the worst-case performance of
  quick-sort.}
\item
  \textbf{Provide a rigorous proof for the expected running time of
  quick-sort.}
\end{enumerate}

\subsection{Computing Medians and Other Order Statistics, Linear-time
Sorting
Solutions}\label{computing-medians-and-other-order-statistics-linear-time-sorting-solutions}

\subsubsection{Practice questions}\label{practice-questions-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{How can we efficiently compute the median of a set of numbers?
  What is the running time of this solution?}
\item
  \textbf{What is the main idea behind counting sort? Provide a
  description of the algorithm and argue about its running time.}

  \begin{quote}
  the algorithm loops over the items, computing a histogram of the
  number of times each key occurs within the input collection. It then
  performs a prefix sum computation (a second loop, over the range of
  possible keys) to determine, for each key, the starting position in
  the output array of the items having that key. Finally, it loops over
  the items again, moving each item into its sorted position in the
  output array.
  \end{quote}

\begin{verbatim}
# variables:
#    input -- the array of items to be sorted; key(x) returns the key for item x
#    n -- the length of the input
#    k -- a number such that all keys are in the range 0..k-1
#    count -- an array of numbers, with indexes 0..k-1, initially all zero
#    output -- an array of items, with indexes 0..n-1
#    x -- an individual input item, used within the algorithm
#    total, oldCount, i -- numbers used within the algorithm

# calculate the histogram of key frequencies:
for x in input:
    count[key(x)] += 1

# calculate the starting index for each key:
total = 0
for i in range(k):   # i = 0, 1, ... k-1
    oldCount = count[i]
    count[i] = total
    total += oldCount

# copy to output array, preserving order of inputs with equal keys:
for x in input:
    output[count[key(x)]] = x
    count[key(x)] += 1

return output
\end{verbatim}

  Because the algorithm uses only simple for loops, without recursion or
  subroutine calls, it is straightforward to analyze. The initialization
  of the Count array, and the second for loop which performs a prefix
  sum on the count array, each iterate at most k + 1 times and therefore
  take O(k) time. The other two for loops, and the initialization of the
  output array, each take O(n) time. Therefore the time for the whole
  algorithm is the sum of the times for these steps, O(n + k).

  Because it uses arrays of length k + 1 and n, the total space usage of
  the algorithm is also O(n + k).{[}1{]} For problem instances in which
  the maximum key value is significantly smaller than the number of
  items, counting sort can be highly space-efficient, as the only
  storage it uses other than its input and output arrays is the Count
  array which uses space O(k).
\item
  \textbf{What is the main idea behind radix sort? Provide a description
  of the algorithm and argue about its running time.}

  \emph{Radix sort} works by first sorting by the least significant bit,
  then the next most significant bit, all the way to the most
  significant bit.
\item
  \textbf{Prove the correctness of radix sort.}

  For a value $r \le b$, we view each key as having
  $d = \lceil b / r \rceil$ digits of $r$ bits each. Each digit is an
  integer in the range of 0 to $2^r - 1$, so that we can use counting
  sort with $k = 2^r - 1$. Each pass of counting sort takes time
  $\Omega(n + k) = \Omega(n + 2^r)$ and there are $d$ passes. This makes
  a total running time of $\Omega(d(n + 2^r))$.
\item
  \textbf{What is the main idea behind bucket sort? Provide a
  description of the algorithm. Prove its running time.}

  \begin{itemize}
  \item
    Idea

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Set up an array of initially empty ``buckets''.
    \item
      Scatter: Go over the original array, putting each object in its
      bucket.
    \item
      Sort each non-empty bucket.
    \item
      Gather: Visit the buckets in order and put all elements back into
      the original array.
    \end{enumerate}

    \begin{quote}
    The idea behind bucket sort is that if we know the range of our
    elements to be sorted, we can set up buckets for each possible
    element, and just toss elements into their corresponding buckets. We
    then empty the buckets in order, and the result is a sorted list.
    \end{quote}
  \item
    Psuedocode

\begin{verbatim}
function bucketSort(array, n) is
    buckets ← new array of n empty lists
    for i = 0 to (length(array)-1) do
        insert array[i] into buckets[msbits(array[i], k)]

    for i = 0 to n - 1 do
        nextSort(buckets[i]);

    return the concatenation of buckets[0], ...., buckets[n-1]
\end{verbatim}
  \end{itemize}
\end{enumerate}

\subsection{Greedy Algorithms: Huffman
encoding}\label{greedy-algorithms-huffman-encoding}

\subsubsection{Reading material}\label{reading-material-1}

\paragraph{Chapter 5.2 from DPV: Huffman
encoding}\label{chapter-5.2-from-dpv-huffman-encoding}

\begin{itemize}
\item
  In general, how do we find the optimal coding tree, given the
  frequencies $f_1 \cdots f_n$ of $n$ symbols? To make the problem
  precise, we want a tree whose leaves each correspond to a symbol and
  which minimizes the overall length of the encoding. \textbf{Cost of
  tree}, where $d$ is the depth of the $i$th symbol in the tree:

  \[\sum_{i = 1}^n f_i \times d\]
\item
  We can define the frequency of any internal node to be the sum of the
  frequencies of its descendant leaves; this is, after all, the number
  of times the internal node is visit during encoding or decoding.

  \begin{itemize}
  \item
    The total cost can be expressed thusly:

    \begin{quote}
    The cost of a tree is the sum of the frequences of all leaves and
    internal nodes, except the root.
    \end{quote}
  \end{itemize}

\begin{verbatim}
function Huffman(f)
input: Any array f[1 ... n] of frequencies
output: An encoding tree with n leaves

let H be a priority queue of integers, ordered b f
for i = 1 to n: insert(H, i)
for k = n + 1 to 2n - 1:
    i = deletemin(h), j = deletemin(h)
    create a node numbered k with children i, j
    f[k] = f[i] + f[j]
    insert(H, k)
\end{verbatim}
\end{itemize}

\paragraph{Chapter 16.2 from CLRS2: Greedy
algorithms}\label{chapter-16.2-from-clrs2-greedy-algorithms}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Greedy algorithm]
A greedy algorithm always makes the choice that looks best at the
moment. That is, it makes a locally optimal choice in the hope that this
choice will lead to a globally optimal solution.
\end{description}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Greedy algorithms do not always yield optimal solution, but for many
  problems they do.
\item
  The following steps apply to a greedy algorithm strategy:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Determine the optimal substructure of the problem.
  \item
    Develop a recursive solution.
  \item
    Prove that at any stage of the recursion, one of the optimal choices
    is the greedy choise. Thus, it always safe to make the greedy
    choice.
  \item
    Show that all but one of the subproblems induced by having made the
    greedy choice are empty.
  \item
    Develop a recursive algorithm that implements the greedy strategy.
  \item
    Convert the recursive algorithm to an iterative algorithm.
  \end{enumerate}
\end{itemize}

\paragraph{Wikipedia: Greedy
algorithms}\label{wikipedia-greedy-algorithms}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A greedy algorithm is an algorithm that follows the problem solving
  heuristic of making the locally optimal choice at each stage with the
  hope of finding a global optimum.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    For instance, to solve the travelling salesman problem with a greedy
    strategy, adopt the following strategy: ``At each stage visit an
    unvisited city nearest to the current city.''
  \end{itemize}
\item
  There are five components:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    A candidate set, from which a solution is created.
  \item
    A selection function, which chooses the best candate to be added to
    the solution.
  \item
    A feasibility function, that is used to determine if a candidate can
    be used to contribute to a solution.
  \item
    An objective function, which assigns a value to a solution, or a
    partial solution, and
  \item
    A solution function, which will indicate when we have discovered a
    complete solution.
  \end{enumerate}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the main principle behind greedy algorithms?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The main principle behind greedy algorithms is to always, in any
    given decision point, to select the locally optimal choice and hope
    that it leads to a globally optimal choice (which has no guarentee).
  \end{itemize}
\item
  \textbf{Why does the greedy algorithm work for the coin changing
  problem given the US coin system?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The US coin system placed in a set of integers has the property of
    being a \textbf{matroid}.
  \end{itemize}
\item
  \textbf{What is the idea in Huffman encoding in order to achieve data
  compression? What is the prefix-free property?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Huffman oding is an entropy encoding algorithm used for lossless
    data compression.
  \item
    The term refers to the use of a variable-length code table for
    encoding a source symbol (such as a character in a file) where the
    variable-length code table has been derived in a particular way
    based on the estimated probability of occurance for each possible
    value of the source symbol.
  \end{itemize}
\item
  \textbf{Provide the Huffman encoding algorithm and argue its running
  time.}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Create a leaf node for each symbol and add it to the priority queue.
  \item
    While there is more than one node in the queue:

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Remove the two nodes of highest priority (lowest probability) from
      the queue.
    \item
      Create a new internal node with these two nodes as children and
      with probability equal to the sum of the two nodes probabilities.
    \item
      Add the new node to the queue.
    \end{enumerate}
  \item
    The remaining node is the root node and the tree is complete.
  \end{enumerate}
\item
  \textbf{Prove the greedy choice property for the Huffman encoding
  algorithm (first lemma).}

  \begin{quote}
  \textbf{\emph{Lemma 16.2}} Let $C$ be an alphabet in which each
  character $c \in C$ has frequency $f[c]$. Let $x$ and $y$ be two
  character in $C$ having the lowest frequencies. Then there exists an
  optimal prefix code $C$ in which the codewords for $x$ and $y$ have
  the same length and differ only in the last bit.
  \end{quote}
\item
  \textbf{Prove the optimal substructure property for the Huffman
  encoding algorithm (second lemma).}
\end{enumerate}

\section{Midterm 2 Study Guide}\label{midterm-2-study-guide}

\subsection{Greedy Algorithms: Horn formulas, Set
Cover}\label{greedy-algorithms-horn-formulas-set-cover}

\subsubsection{Chapter 5.2 from DPV: Huffman
encoding}\label{chapter-5.2-from-dpv-huffman-encoding-1}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Is there some sort of \emph{variable-length encoding} in which just
  one bit is used for the most frequent symbol and the least amout of
  bits are used for the lesser freqeuent symbols?
\item
  One danger of this is that resulting encodings might not be uniquely
  decipherable.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    For instance, considering $\lbrace 0, 01, 11, 001 \rbrace$. Strings
    like $001$ are ambigous.
  \item
    We need the \emph{prefix-free} property.
  \end{itemize}
\end{itemize}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Prefix free]
No codeword can be a prefix of another codeword.
\end{description}

\[\mathrm{cost \: of \: tree} = \sum_{i=1}^n f_i \cdot (\mathrm{depth \: of ith \: symbol \: in \: tree})\]

\begin{itemize}
\item
  The cost of a tree is the sums of the frequencies of all leaves and
  internal nodes, except the root.
\item
  We can state constructing the tree \emph{greedily}: find two symbols
  of smallest frequencies, $i$ and $j$, and make them children of a new
  node, which has the frequency $f_i + d_j$.

\begin{verbatim}
procedure Huffman(f)
Input: an array of frequencies
Output: an encoding tree

let H be a priority queue of integers, ordered by f 
for i = 1 to n {
    insert(H,i)
}
for k = n + 1 to 2n - 1 {
    i = deletemin(H); 
    j = deletemin(H);
    create a node numbered k with children i, j
    f[k] = f[i] + f[j]; 
    insert(H, k)
}
\end{verbatim}
\end{itemize}

\subsubsection{Chapter 5.3 from DPV: Horn
formulas}\label{chapter-5.3-from-dpv-horn-formulas}

\begin{itemize}
\item
  To display human intelligence, a computer must do some logical
  reasoning.
\item
  Most primitive object in Horn formulas is the \emph{Boolean variable},
  which can be \texttt{true} or \texttt{false}. Ex,
  \[x \equiv \text{the murder took place in the kitchen}\]
  \[y \equiv \text{the butler is innocent}\]
  \[z \equiv \text{the colonel was asleep at 8pm}\]
\item
  A \emph{literal} is either a variable $x$ or its negation $\lnot x$.
  In Horn formulas, knowledge about variables is represented in two
  kinds of \emph{clauses}:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \emph{Implications}, whose left-hand is an ``and'' of any number of
    positive literals and whose right-hand side is a single positive
    literal. For instance, \[(z \land w) \to u\]

    ``If the colonel was asleep at 8 pm and the murder took place at
    8pm, then the colonel is innocent.''
  \item
    Pure \emph{negative cluases}, consisting of an ``or'' of any number
    of negative literals, as in, \[(\lnot u \lor \lnot v \lnot y)\]

    ``They can't all be innocent.''
  \end{enumerate}
\item
  Given a set of these two types, the goal is to determine whetheter
  there is a consistent explanation, an assignment of true/false values
  to the variables that satisfies all the cluases. \emph{Satisfaction
  requirement}.
\end{itemize}

\subsubsection{Chapter 5.4 from DPV: Set
cover}\label{chapter-5.4-from-dpv-set-cover}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Given a set of towns and these two constraints: (1) each school should
  be in a town, and (2) no school should have to travel more than 30
  miles to reach a school.
\item
  This is a \emph{set cover} problem.
\end{itemize}

\begin{quote}
Set cover

\emph{Input}: A set of elements $B$; sets $S_1, \cdots, S_m \in B$

\emph{Output}: A selection of the $S_i$ whose union is $B$.

\emph{Cost}: Number of sets picked
\end{quote}

\begin{itemize}
\item
  There is a greedy solution:

\begin{verbatim}
While all elements B are uncovered:
    Pick the set $S_i$ with the largest number of uncovered elements
\end{verbatim}
\end{itemize}

\begin{quote}
\textbf{Claim}: Suppose $B$ contains $n$ elements and the algorithm
consists of $k$ sets. Then the greedy algorithm will use at most
$k \ln n$ sets.
\end{quote}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The ration between the greedy algorithm's solution and the optimal
  solution varies from iinput to input but is \emph{way less} than
  $\ln n$
\end{itemize}

\subsubsection{Chapter 16.2 from CLRS2: Elements of the greedy
strategy}\label{chapter-16.2-from-clrs2-elements-of-the-greedy-strategy}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A greedy algorithm obtains an optimal solution to a problem by making
  choices.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Every choice, the choice is the best \emph{at the moment}.
  \item
    This doesn't always work, but sometimes it does.
  \end{itemize}
\item
  The steps:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Determine the optimal substructure of the problem.
  \item
    Develop a recursive solution.
  \item
    Prove that at any stage of the recusion, one of the optimal choices
    is the greedy choice. That is is always safe to make the greedy
    choice.
  \item
    Show that all but one of the subproblems induces by having made the
    greedy\\ choice are empty.
  \item
    Develop a recursive algorithm that implements the greedy strategy.
  \item
    Convert the recrusive algorithm to an iterative algorithm.
  \end{enumerate}
\item
  The steps, more generally:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Cast the optimization problem as one in which we make a choice and
    are left with one subproblem to solve.
  \item
    Prove that there is always an optimal solution to the original
    problem that makes the greedy choice, so that the greedy choice is
    not always safe.
  \item
    Demonstrate that, having made the greedy choice, what remains is a
    subproblem with the property that is we combine an optimal solution
    to the subprobelm with the greedy choice we have made, we arrive at
    an optimal solution to the original problem.
  \end{enumerate}
\end{itemize}

\paragraph{Greedy-choice property}\label{greedy-choice-property}

\begin{quote}
A globally optimal solution can be arrived at by making a locally
optimal (greedy) choice.
\end{quote}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  How greedy algorithms differ from dynamic programming:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    In dynamic programming, we make a choice at each step, but the
    choice usually depends on the solutions to subproblems. It is
    ``bottom'', going from smaller subproblems.
  \item
    In a greedy algorithm, we make a choice that is best at the moment
    and then solve the subproblem arising after choice is made.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Greedy algorithms are ``top-down.''
    \end{itemize}
  \end{itemize}
\end{itemize}

\paragraph{Optimal substructure}\label{optimal-substructure}

\begin{quote}
If an optimal solution to the porblem contains within it optimal
solutions subproblems.
\end{quote}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  This is key to greedy algorithms and dynamic programming.
\end{itemize}

\subsubsection{Chapter 16.3 from CLRS2: Huffman
codes}\label{chapter-16.3-from-clrs2-huffman-codes}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Prefix codes]
Codes in which no codeword is also a prefix of some other codeword.
\end{description}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  In the following psuedocode, assume $C$ is a set of $n$ characters and
  that each character $c \in C$ is an object with a defined frequency
  $f[c]$.

  \begin{itemize}
  \item
    The algorithm building the tree $T$ corresponding to the optimal
    code in bottom-up manner.

\begin{verbatim}
Huffman(C)
n <- |C|
Q <- C
for i <- to n - 1:
    do allocate a new node z
        left[z] <- x <- Extract-Min(Q)
        right[z] <- y <- Extract-Min(Q)  
        f[z] <-  f[x] + f[y]
        Insert(Q, z)

return Extract-Min(Q)
\end{verbatim}
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the main principle behind greedy algorithms?}

  The principle behind greedy algorithms is that at every point where a
  given algorithm can make a choice, instead of trying to figure out if
  there exists a ``globally optimal'' decision, a greedy algorithm just
  goes with the cheapest option in that moment and ``hopes for the
  best.''
\item
  \textbf{Why does the greedy algorithm work for the coin changing
  problem given the US coin system?}

  The reason that the US coin system allows the greedy choice to
  solvethe coin change problem is that a matroid can be formed from the
  set of US coins.
\item
  \textbf{What is the idea in Huffman encoding in order to achieve data
  compression?}

  The idea of Huffman encoding is that you take the characters which
  appear most often in your document and you make those the cheapest to
  encode. You subsequently take the least frequent characters in your
  document and make them only as expensive as the prefix-free property
  demands they be.
\item
  \textbf{What is the prefix-free property?}

  The prefix-free property is that for a given dictionary, no word in
  the dictionary has at its beginning any other word.
\item
  \textbf{Provide the Huffman encoding algorithm and argue its running
  time.}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Create a leaf node for each symbol and add it to the priority queue.
  \item
    While there is more than one node in the queue:

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Remove the two nodes of lowest probability.
    \item
      Crate a new node with the sum of those two probabilities.
    \item
      Add that node to the queue.
    \end{enumerate}
  \item
    The remaining node is the root node and the tree is complete.
  \end{enumerate}

  \begin{quote}
  Since efficient priority queue data structures require $O(log n)$ time
  per insertion, and a tree with $n$ leaves has $2n - 1$ nodes, this
  algoirhtm operates in $O(n \log n)$ time, where $n$ is the number of
  symbols.
  \end{quote}
\item
  \textbf{Describe the greedy algorithm for logical reasoning with Horn
  formulas.}

\begin{verbatim}
Input: a horn formula
Output: a satisfying assignment, if one exists

set all variables to false
while there is an implication that is not satisfied:
    set the right-hand variable of the implication to true

if all pur negative clauses are satisfied: return the assignment
else: return ``formula is not satisfiable``
\end{verbatim}
\item
  \textbf{What is the property of the greedy algorithm for the set cover
  example? Prove it.}
\end{enumerate}

\subsection{Elements of Dynamic Programming, Matrices,
Subsequences}\label{elements-of-dynamic-programming-matrices-subsequences}

\subsubsection{Chapter 6.2 from DPV: Longest increasing
subsequences}\label{chapter-6.2-from-dpv-longest-increasing-subsequences}

\begin{itemize}
\item
  In the \emph{longest increasing subsequence} problem, the input is a
  sequence of numbers $a_1, \cdots, a_n$.

  \begin{itemize}
  \item
    A \emph{subsequence} is any subset of these number taken in order,
    of the form $a_{i_1}, a_{i_2}, \cdots, a_{i_k}$ where
    $1 \ge i_1 < i_2 < ... < i_k \le n$ and an \emph{increaing}
    subsequence is one in which the numbers are getting strictly larger.
  \item
    For instance, consider the following sequence:
    \[5 \: 2 \: 8 \: 6 \: 3 \: 6 \: 9 \: 7\]

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      The longest increasing subsequence is 2, 3, 6, 9.
    \end{itemize}
  \end{itemize}
\item
  You can express this problem as a graph problem where the graph is a
  directored acyclic one, notice two properties:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    It's a DAG
  \item
    There is a one-to-one correspondance between increasing subsequences
    and paths in this dag.
  \end{enumerate}
\item
  Just find the longest path in the dag!

\begin{verbatim}
for j = 1, 2, ..., n:
    L[j] = 1 + max{L(i) : (i, j) in E}
return max_j L(j)
\end{verbatim}
\item
  This is dynamic programming. We have defined a collection of
  subproblems with the following property that allows them to solved in
  a single pass:

  \begin{quote}
  (*) There is an ordering on the subporblems, and a relation that shows
  how to solve a subproblem given the answer to ``smaller'' subproblems,
  that is, subproblems that appear earlier in the ordering.
  \end{quote}
\item
  Each subproblem is solved using the relation:
  \[L(j) = 1 + max \lbrace L(i) : (i, j) \in E \rbrace\]
\end{itemize}

\subsubsection{Chapter 6.3 from DPV: Edit
distance}\label{chapter-6.3-from-dpv-edit-distance}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  When a spell-checker encounters a possible misspelling, it looks in
  its dictionary for other words that are close by. What is the
  appropriate notion of closeness in this case?

  \begin{itemize}
  \item
    Perhaps how much they can be \emph{aligned} or \emph{matched-up}?

\begin{verbatim}
S - N O W Y   - S N O W - Y
S U N N - Y   S U N - - N Y
\end{verbatim}
  \end{itemize}
\end{itemize}

\paragraph{A dynamic programming
solution}\label{a-dynamic-programming-solution}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Doing this, the most crucial question is: \emph{What are the
  subproblems?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    As long as they have the propert (*), it is easy to write down the
    algorithm: iteratively solve one problem after the other in order of
    increasing size.
  \end{itemize}
\item
  Our goal is to find the distance between two strings. What is a good
  subproblem?

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Find the prefix of the first and second strings.
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 15.2 from CLRS2: Matrix-chain
multiplication}\label{chapter-15.2-from-clrs2-matrix-chain-multiplication}

\begin{itemize}
\item
  An example of \emph{dynamic programming}.

\begin{verbatim}
MatrixMultiply(A, B) {
    if columns(A) != columns(B)
        then error "incompatible dimensions"
        else for i <- 1 to rows[A]
            do for j <- 1 to columns[B] 
                do C[i, j] <- 0
                    for k <- 1 to clumns[A]
                        do C[i, j] <- C[i, j] + A[i, k] * B[k, j]
        return C
}
\end{verbatim}
\end{itemize}

\subsubsection{Chapter 15.3 from CLRS2: Elements of Dynamic
Programming}\label{chapter-15.3-from-clrs2-elements-of-dynamic-programming}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The elements of dynamic programming: optimal substructure and
  overlapping subproblems.
\end{itemize}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Optimal substructure]
If an optimal solution to the poroblem contains within it optimal
solutions to subproblems.
\end{description}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  How to find it, some characteristic:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    You should that a solution to the problem consists of making
    choices, where making a choice leaves a subproblem to be solved.
  \item
    You suppose that for a given problem, you are given the choice that
    leads to an optimal solution. You do not concern yourself yet with
    how to determine this, you just assume it's been given to you.
  \item
    Given this choice, you determine which subproblems ensue and how to
    best characterize the result space of subproblems.
  \item
    You show that the solutions to the subprobelsm used within the
    optimal\\ solution to the problem must themselves be optimal. You do
    so by supposing otherwise and deriving a contradiction.
  \end{enumerate}
\item
  Optimal substructure differes in two ways:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    How many subproblems are used in an optimal solution to the original
    problem
  \item
    How many choices we have in determineing which subporblems to use in
    an optimal solution.
  \end{enumerate}
\end{itemize}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Overlapping subproblems]
When a recursive algorithm revists the same problem over and over again,
we say that the optimization problem has \emph{overlapping subproblems}.
\end{description}

\subsubsection{Practice questions}\label{practice-questions-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{How does the dynamic programming approach decompose the
  chain-matrix multiplication into subproblems and how are their
  solutions combined in order to solve a larger problem?}

  \begin{quote}
  Suppose that an optimal parenthesization of $A_i A_{i + 1} \cdots A_j$
  splits the product between $A_k$ and $A_{k + 1}$. Then the
  parenthesization of the ``prefix'' subchain $A_i A_{i + 1} \cdots A_k$
  whithin this optimal parenthesization of $A_i A_{i + 1} \cdots A_j$
  must be an optimal parenthesization of $A_i A_{i + 1} \cdots A_{k}$.
  Why? If there were a less costly way to parenthesize
  $A_i A_{i + 1} \cdots A_k$ substituting that parenthesization in the
  optimal parenthesization of $A_i A_{i + 1} \cdots A_j$ would produce
  another parenthesization of $A_i A_{i + 1} \cdots A_j$ whose cost was
  lower than optimum: a contraduction. A similar observation holds for
  the parenthesization of the subchain $A_{k + 1} A_{k + 2} \cdots A_j$
  in the optimal parenthesization of $A_i A_{i + 1} \cdots A_j$: it must
  be an optimal parenthesization of $A_{k + 1} A_{k + 2} \cdots A_j$.
  \end{quote}
\item
  \textbf{What is the dynamic programming algorithm for chain-matrix
  multiplication? What is its running time?}

  \begin{quote}
  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Take the sequence of matrices and separate it into two subsequences.
  \item
    Find the minimum cost of multiplying out each subsequence.
  \item
    Add these costs together, and add in the cost of multiplying the two
    result matrices.
  \item
    Do this for each possible position at which the sequence of matrices
    can be split, and take the minimum over all of them.
  \end{enumerate}
  \end{quote}
\item
  \textbf{You may be asked to compute a product of matrices using the
  dynamic programming approach and report the number of operations
  required given the dimensionality of the input matrices.}
\item
  \textbf{What type of subproblems does the dynamic programming approach
  consider for solving the longest increasing subsequence and how does
  it combine them in order to solve larger ones?}

  The subproblems for the LIS problem as the suffixes for any given
  input. Find the longest increasing subsequence $A[j..n]$ that includes
  $A[j]$. Do this for each $j > i$.
\item
  \textbf{What is the running time of the dynamic programming solution
  for the longest increasing subsequence problem?}

  There are $n$ subproblems. For every subproblem $i$, the worst case is
  that you need go through all the following items to get the maximum,
  which is $O(n - i)$

  \[\sum_{i=1}^n O(i) = O\left (\sum_{i=1}^n i \right ) = O\left ( \dfrac{n(n-1)}{2} \right ) = O(n^2)\]
\item
  \textbf{You may be given an example sequence and asked to compute its
  longest increasing sub- sequence following a dynamic programming
  approach. Your solution should provide the intermediate values
  computed by the algorithm (e.g., length of longest subsequence,
  previous pointer for each digit).}
\item
  \textbf{What are the subproblems defined by the dynamic programming
  approach for the longest common subsequence problem and how are they
  combined to solve larger problems?}

  \begin{quote}
  Let $X = \lbrace x_1, x_2, \cdots, x_m \rbrace$ and
  $Y = \lbrace y_1, y_2, \cdots, y_n \rbrace$, and let
  $Z = \lbrace z_1, z_2, \cdots, z_k$ be any LCS of $X$ and $Y$.

  Three cases:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    If $x_m = y_n$, then $z_k = x_m = y_n$ and $Z_{k - 1}$ is an LCS of
    $X_{m - 1}$ and $Y_{n - 1}$.
  \item
    If $x_m \not = y_n$, then $z_k \not = x_m$ implies that $Z$ is an
    LCS of $X_{m - 1}$ and $Y$.
  \item
    If $x_m \not = y_n$, then $z_k \not = x_m$ implies that $Z$ is an
    LCS of $X$ and $Y_{n - 1}$.
  \end{enumerate}
  \end{quote}
\item
  \textbf{What is the running time of the dynamic programming solution
  for the longest common subsequence problem?}

  To build th LCS, you must construct a table with $m$ columns and $n$
  rows. Each entry into the table is constant time.

  \[O(m \times n)\]
\item
  \textbf{You may be given two strings and asked to compute their
  longest common subsequence following a dynamic programming approach.
  Your solution should provide the intermediate values computed by the
  algorithm (e.g., the matrix of values corresponding to the cost of the
  matching and the parent pointers).}
\end{enumerate}

\subsection{Elements of Dynamic Programming, Knapsack problem, Graph
Representation}\label{elements-of-dynamic-programming-knapsack-problem-graph-representation}

\subsubsection{Chapter 6.4 from DPV:
Knapsack}\label{chapter-6.4-from-dpv-knapsack}

\begin{quote}
During a robbery, a burglar finds much more loot than he had expected
and has to decide what to take. His knapsack will hold a total weight of
\emph{at most} $W$ pounds. There are $N$ items to pick from, of which
$w_1, \cdots, w_n$ and dollar value $v_1, \cdots, v_n$. What's the most
valuable combination he can fit in his bag?
\end{quote}

\paragraph{Knapsack with repetition}\label{knapsack-with-repetition}

\begin{itemize}
\item
  What are the subproblems? We can look at it two ways:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    We can look at smaller knapsack capacities $w \le W$
  \item
    We can look at fewer items (for instance, items $1, 2, \cdots, j$,
    for $J \le n$)
  \end{enumerate}
\item
  The first restriction, roughly,

  \begin{quote}
  $K(w)$ is the maximum achievable value with a knapsack of capacity
  $w$.
  \end{quote}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Can we express this in terms of smaller subproblems? Well, if you
    remove an item $i$ from $K(w)$ In other words,
    $K(w) = K(w - w_i) + v_i$
  \end{itemize}
\item
  If the optimal solution to $K(w)$ includes the item $i$, then removing
  this item from the knapsack leaves an optimal solution to

  \[K(w) = \text{max}_{i : w_i \le w} \lbrace K(w  w_i) + v_i \rbrace\]

\begin{verbatim}
K(0) = 0
for w = 1 to W:
    k(w) = max \{ K(w - w_i) + v_i \}
return K(W)
\end{verbatim}
\end{itemize}

\paragraph{Knapsack without
repetition}\label{knapsack-without-repetition}

\begin{quote}
$K(w, j)$ is the maximum value achievable using a knapsack of capacity
$w$ and items $1, \cdots, j$.
\end{quote}

\begin{itemize}
\item
  Either item $j$ is needed to achieve optimal value, or it isn't
  needed:
  \[K(w, j) = max \lbrace K(w - w_j, j - 1) + v_j, K(w, j - 1) \rbrace\]

\begin{verbatim}
Initmizalize all K(0, j) = 0 and all K(w, 0) = 0
for j = 1 to n:
    for w= 1 to W:
        if w_j > w: K(w, j) = K(w, j - 1)
        else: K(w, j) = max(K(w, j - 1), K(w - w_j, j - 1) + v_j)

return K(W, n)
\end{verbatim}
\end{itemize}

\subsubsection{Chapter 3.1 from DPV: Why
graphs?}\label{chapter-3.1-from-dpv-why-graphs}

\begin{itemize}
\item
  We can represent a graph with an \emph{adjacency matrix}, if there are
  $n = |V|$ vertice $v_1, ..., v_n$, this is an $n \times n$ array whose
  $(i, j)$th entry is:

  \[a_{ij} = \begin{cases} 1, & \text{if there is an edge } v_i \to v_j \\ 0, & \text{otherwise}. \end{cases}\]

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The biggest convinience of this is that an edge can be checked in
    constant time.
  \item
    However, it takes $O(n^2)$ space, wasteful if not many edges.
  \end{itemize}
\item
  An alternative representation is an \emph{adjacency list}.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    It consists of $|V|$ linked lists, one per vertex.
  \item
    Each edge appear in exactly one of the linked lists if the graph is
    directed or two of the lists if the graph is undirected.
  \item
    The total size of the data structure is $O(|E|)$.
  \item
    Checking an edge is no longer linear, sift through $u$'s adjacency
    list.
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 22.1 from CLRS2: Representations of
Graphs}\label{chapter-22.1-from-clrs2-representations-of-graphs}

\begin{itemize}
\item
  A graph is sparse if $|E|$ is much less than $|V|^2$.
\item
  A graph is dense if $|E|$ is close to $|V|^2$.
\item
  Adjency list representations of graphs consist of any array of $|V|$
  lists, one for each vertex in $V$.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The adjacency list $Adj[u]$ contains all the vertices $v$ such that
    there is an edge $(u, v) \in E$.
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What are the subproblems defined by the dynamic programming
  approach for the knapsack problem with repetition how are they
  combined to solve larger problems? What is the runtime?}

  Define $K(w)$ as the maximum value achievable with a knapsack of
  capacity $w$. The subproblems, then, where $v$ is value of item $w$ at
  $i$:

  \[K(w) = \max_{i:w_i \le w} \lbrace K(w - w_i) + v_i \rbrace\]

  The algorithm falls nicely out:

\begin{verbatim}
K(0) = 0;
for w = 1 to W:
    K(w) = max(K(w - w_i) + v_i : w_i <= w)
return K(W)
\end{verbatim}

  Each entry can tke up to $O(n)$ time to compute, so the overall
  running time is

  \[O(nW)\]
\item
  \textbf{What are the subproblems defined by the dynamic programming
  approach for the knapsack problem without repetition how are they
  combined to solve larger problems? What is the runtime?}

  Define $K(w, j)$ as the ``maximum value chieable using a knapsack of
  capacity $w$ and items $1, \cdots, j$.'' The answer we seek is
  $K(W,n)$. The subproblems, therefore, are where $j$ is either needed
  or not:

  \[K(w, j) = \max \lbrace L(w - w_j, j - 1) + v_j, K(w, j - 1) \rbrace\]

  The algorithm consists of filling out a two dimesional table, with
  $W + 1$ rows and $n + 1$ columns. Each table entry is constant time,
  even though the table is much larger than in the previous case, the
  running time is still:

  \[O(nW)\]
\item
  \textbf{You may be given the parameters of a knapsack problem and
  asked to compute the answer following a dynamic programming approach
  (for either case: with or without repetition). Your solution should
  provide the intermediate values computed by the algorithm (e.g., the
  vector or matrix of intermediate payoffs and objects selected by the
  intermediate solutions).}
\item
  \textbf{What are the advantages and disadvantages of an adjacency
  matrix representation for graphs?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Advantages:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Constant time checking of edges.
    \end{itemize}
  \item
    Disadvantages:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Requires $O(n^2)$ space no matter how dense or sparse.
    \end{itemize}
  \end{itemize}
\item
  \textbf{What are the advantages and disadvantages of an adjacency list
  representation for graphs?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Advantages:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Less space than matrix, very small for sparse graphs.
    \end{itemize}
  \item
    Disadvantages:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      For dense graphs, look up for edges will be costly, linear.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\subsection{Depth-First Search}\label{depth-first-search}

\subsubsection{Chapter 3.2 from DPV}\label{chapter-3.2-from-dpv}

\begin{itemize}
\item
  Depth-first search is a verstile linear-time procedire that reveals a
  wealth of information about a graph.

  \begin{quote}
  What parts of the graph are reachable from a given vertex?
  \end{quote}

\begin{verbatim}
procedure explore(G, v) {
    input:  G=(V, E) is a graph; v in V
    output: visited(u) is set to true for all nodes u reachable from v

    visited(v) = true;
    previsit(v);

    for each edge(v, u) in E:
        if not visited(v): explore(u);

    postvisit(v);
}

procedure dfs(G) {
    for all v in V
        visited(v) = false;

    for all v in V
        if not visisted(v): explore(v);
}
\end{verbatim}
\item
  Analysis:

  \begin{itemize}
  \item
    To mark all as unvisisted:

    \[O(|V|)\]
  \item
    Each edge is exacmied exactly twice,

    \[O(|E|)\]
  \item
    Therefore,

    \[O(|V| + |E|)\]
  \end{itemize}
\end{itemize}

\paragraph{Connectivity in undirected
graphs}\label{connectivity-in-undirected-graphs}

\begin{itemize}
\item
  An undirected graph is connected if there is a path between any pair
  of vertices.
\item
  In unconnected graphs, there are at least two connected components.
\item
  All it takes to make DFS assign connected component numbers to each
  on,

\begin{verbatim}
procedure previsit(v) {
    ccnum[v] = cc
}
\end{verbatim}
\end{itemize}

\subsubsection{Chapter 22.5 from CLRS2}\label{chapter-22.5-from-clrs2}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  An application for DFS: decomposisiting a graph into strongly
  connected components.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    To be a strongly connected component of a directed graph is to have
    a maxmimal set $C \subset V$ such that for every pair of vertices
    $u$ and $v$ in $C$, we have both a path from $u$ to $v$ and vice
    versa.
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Provide an algorithm that discovers in linear time the set of
  nodes in a graph reachable from a specific node. Argue about its
  correctness.}

\begin{verbatim}
function get_connected_nodes(node){
    nodes = set();
        foreach(child in node.children){
                nodes.add(child);
                        nodes = nodes.union(get_connected_nodes(child));
                            }
                                return nodes;
                                }
        }
}
\end{verbatim}
\item
  \textbf{Provide an algorithmic description for depth-first search.
  What is its running time?}

  Depth first search operates by first marking all vertexes in a graph
  as unvisisted. Then, it travereses all the vertexes in the graph. If a
  vertex has not been visisted, that vertex is explored. When a vertex
  is explored, it is set to have been viststed, and subseqeuntly every
  vertex that it shares and edge with is explored as well (that is,
  recursively). Optionally, the explore function can have a previsit and
  postvisit operation.

  There are two important elements to an analysis of depth first search.
  First, there is the marking of every node as unvisisted, which will
  take as much time as there are nodes. Then, note that every edge is
  visisted twice in the explore function. First, when it is found in the
  first node, and second, when its found in a node that looks at it,
  that is, in a undirected graph.

  \[O(|E| + |V|)\]
\item
  \textbf{What is the pre-visit and post-visit order of nodes in
  depth-first search?}

  For any nodes $u$ and $v$, the two intervals $[pre(u), post(u)]$ and
  $[pre(v), post(v)]$ are either disjoint or one is contained within the
  other. Because $[pre(u), post(u)]$ is essentially the time during
  which vertex $u$ was on the stack. The last in, first out behavior of
  the stack.
\item
  \textbf{How can depth-first search be used in order to detect the
  connected components of a graph?}

  If you modify the previsit function of DFS's explore function to
  number the visist node with a number that begins at 0 and is
  incrememented every time that explore function is called in the DFS
  function, you'll get marked nodes with a mark of their component.
\item
  \textbf{You may be provided an undirected or directed graph, a start
  node and asked to provide the search tree arising from a depth-first
  search. You may also be asked to identify tree edges, forward edges,
  back edges and cross edges (directed cases).}
\item
  \textbf{Show that a directed graph has a cycle if and only if its
  depth-first search reveals a back edge.}

  If $(u, v)$ is a back edge, then there is a cycle consisting of this
  edge together with the path from $v$ to $u$ in the search tree.

  If the graph \emph{has} as cycle
  $v_0 \to v_1 \to \cdots \to v_k \to v_0$, look at the first node on
  this cycle to be discovered. Suppose it is $v_i$. All other $v_j$ on
  the cycle are reachable from it and will therefore be its descendants
  in the search tree. In particular, the edge $v_{i - 1} \to v_i$ (or
  $v_k \to v_0$ if $i = 0$) leads from a node to its ancestor and is
  thus by definition a back edge.
\item
  \textbf{What is a directed acyclic graph (DAG)?}

  A cycle in a directed graph is a circular path. A graph without one of
  these is acyclic. If that acyclic graph's edges have direction, then
  it is a DAG.
\item
  \textbf{What is the topological ordering of nodes in a DAG and why it
  is useful?}

  To linearize a DAG, perform DFS and perform tasks in decreasing order
  of their post number.

  Alternatively, find a source, output it, and delete it from the graph.

  It's useful because you can model causalityies, hierarchies, and
  temporal dependancies.
\item
  \textbf{How is it possible to identify a sink node or a source node in
  a DAG using depth-first search?}

  To indentify a sink node in a DAG, perform DFS and when a node doesn't
  have any edges, it will be a sink.

  To identify a source node in a DAG using DFS,
\item
  \textbf{What is the definition of strongly connected components?}

  If two components in a graph have an edge between $u$ and $v$ going in
  both directions, then those components are strongly connected.
\item
  \textbf{How is it possible to compute the decomposition of a directed
  graph into its strongly connected components? Provide an algorithm and
  argue about its running time.}

  The algorithm:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Let $G$ be a directed graph and $S$ be an empty stack.
  \item
    While $S$ does not contain all vertices:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Choose an arbitrary vertex $v$ not in $S$. Perform DFS starting at
      $v$. Each time that DFS finished expanding a vertex $u$, push $u$
      onto $S$.
    \end{itemize}
  \item
    Obtain the transpose of $G$.
  \item
    While $S$ is not empty:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Pop the top vertex $v$ from $S$.
    \item
      Perform DFS starting at $v$ in the transpose graph. The set of
      visisted vertices will give the strongly connected component
      containing $v$.
    \item
      Record this and remove all these vertices from the graph $G$ and
      the stack $S$.
    \end{itemize}
  \end{itemize}

  The run time: This algorithm performs two traversals of the graph.

  \[O(|V| + |E|)\]
\end{enumerate}

\subsection{Breadth-First Search, Properties of Shortest
Paths}\label{breadth-first-search-properties-of-shortest-paths}

\subsubsection{Chapter 4.1 from DPV:
Distances}\label{chapter-4.1-from-dpv-distances}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The \emph{distance} between two nodes is the length of the shortest
  path between them.
\end{itemize}

\subsubsection{Chapter 4.2 from DPV: Breadth-first
search}\label{chapter-4.2-from-dpv-breadth-first-search}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Each vertex is queued exactly once, when it is first encountered, so
  there are $2 |V|$ queue operations.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The rest of the work is done in the algorithm's innermost loop.
  \item
    Over the course of execution, this loop looks at each edge once in
    directed graphs or twice in undirected graphs, and therefore takes
    \$O(\textbar{}E\textbar{}) time.
  \end{itemize}
\item
  Depth first search makes deep uncursion into a graph, retreating only
  it runs out of new nodes to visit.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    BFS makes sure to visist vertices in increasing order of their
    distance from the starting point. This is broader, shallower search,
    rather like the progpagation of a wave upon water.
  \item
    Same as DFS, but with a queue instead of a stack.
  \item
    Nodes not reachable from $s$ are simply ignored.
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 22.2 from CLRS2: Breadth-first
search}\label{chapter-22.2-from-clrs2-breadth-first-search}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  BFS is one of the simplest algorithms for searching a graph.
\item
  Given a graph $G = (V, E)$ and a dinstinguished source vertex $s$, BFS
  systematically explores the edges of $G$ to ``discover'' every vertex
  that is reachable from $s$.
\item
  It's named this because it expands the frontier between discovered and
  undiscovered vertices uniformly across the breadth of the frontier.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Discoveres distances at $k$ before $k + 1$.
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Provide the algorithm that performs breadth-first search on a
  graph.}

\begin{verbatim}
procedure bfs(G, s)
input: Graph G=(V, E) directed or undirected; vertex s in V
output: For all vertices u reachable form s, dist(u) is set to distance
        from s to u.

for all u in V:
    dist(u) = inf

dist(s) = 0
Q = [s] (queue just containing s)
while Q is not empty:
    u = enject(Q)
    for all edges (u, v) in E:
        if dist(v) = inf
            inject(Q, v)
            dist(v) = dist(u) + 1
\end{verbatim}
\item
  \textbf{What is the inductive argument for the correctness of the
  approach?}

  What we expect:

  \begin{quote}
  For each $d = 0, 1, 2 \cdots$, there is a moment at which:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    All nodes at distance $\le d$ from $s$ have their distances
    correctly set;
  \item
    All other nodes have distances set to $\infty$; and
  \item
    The queue contains exactly the nodes at distance $d$.
  \end{enumerate}
  \end{quote}

  \textbf{\emph{Proof}}: Assume, for the purpose of contraduction, that
  some vertex receives a $d$ value not equal to its shortest path
  distance. Let $v$ be th vertex with minimum $\Delta (s, v)$ and
\item
  \textbf{What is the running time of the algorithm?}

  \[O(|V| + |E|)\]

  \begin{quote}
  Each vertex is put on the queue exactly once, when it is first
  encountered, so there are $2|V|$ queue operations. The loop then looks
  at each edge once (or twice in undirected), and therefore takes
  $O(|E|)$ time.
  \end{quote}
\item
  \textbf{What is the definition of a single-source shortest path
  problem?}

  \begin{quote}
  in which we have to find shortest paths from a source vertex v to all
  other vertices in the graph.
  \end{quote}
\item
  \textbf{What is the definition of a single-destination shortest path
  problem?}

  \begin{quote}
  in which we have to find shortest paths from all vertices in the
  directed graph to a single destination vertex v.
  \end{quote}

  \textbf{Protip}: Reverse the direction of SSSP.
\item
  \textbf{What is the definition of a single-pair shortest path
  problem?}

  \begin{quote}
  in which we have to find the shortest path from source $s$ to
  destination $d$.
  \end{quote}
\item
  \textbf{What is the definition of all-pairs shortest path problem?}

  \begin{quote}
  in which we have to find shortest paths between every pair of vertices
  v, v' in the graph.
  \end{quote}
\item
  \textbf{Which of these problems are equivalent?}

  None of them?
\item
  \textbf{Do we know faster algorithms for the single-pair shortest path
  problem than the single-source shortest path problem?}

  Yes, single pair will be faster than single-source, single source has
  to compute more.

  Djikstra's: $O(|E| + |V| \log|V|)$

  Bellman-Ford: $O(|V| \times |E|)$
\item
  \textbf{What is the ``optimal substructure'' property of shortest
  paths? Prove its validity.}

  If you split a shortest path in half, the shortest path from the
  source to halfway will be the first half of the shortest path to the
  destination.
\item
  \textbf{What happens with shortest paths on graphs that contain
  negative cycles? Can a shortest path contain positive cycles?}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    There are going to be infinitely many shortest paths, you can always
    go around that vertex more than once.
  \item
    Yes.
  \end{enumerate}
\end{enumerate}

\subsection{Dijkstra's algorithm}\label{dijkstras-algorithm}

\subsubsection{Chapter 4.3 from DPV: Lengths on
edges}\label{chapter-4.3-from-dpv-lengths-on-edges}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  BFS treats all edges as having the same length. This is rarely true in
  applications where shortest paths are to be found.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Annotating every edge $e \in E$ with length $l_e$.
  \item
    If $e = (u, v)$, we will sometimes also write $l(u, v)$ or $l_{uv}$
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 4.4 from DPV:
Dijkstra}\label{chapter-4.4-from-dpv-dijkstra}

\subsubsection{Chapter 4.5 from DPV: Priority queue
implementations}\label{chapter-4.5-from-dpv-priority-queue-implementations}

\paragraph{Array}\label{array}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The simplimest implementation.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Key values for all potential elements.
  \end{itemize}
\item
  \texttt{insert} and \texttt{decreasekey} is fast, because it just
  involves adjusting the key value.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \texttt{deletemin} requires a scan of the list.
  \end{itemize}
\end{itemize}

\paragraph{Binary heap}\label{binary-heap}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \texttt{insert}: place a new element at the bottom of the tree and let
  it ``bubble up'', where the number of switches is at most $\log n$,
  height of the tree.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \texttt{decrease key} is similar.
  \end{itemize}
\item
  \texttt{deletemin} requires deleting the root.
\end{itemize}

\subsubsection{Chapter 24.3 from CLRS2: Dijkstra's
algorithm}\label{chapter-24.3-from-clrs2-dijkstras-algorithm}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Solves the single single-source shortest path problem on a weighted
  directed graph $G = (V, E)$ for the case where the edge-weights are
  non-negative.
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Provide Dijkstra's algorithm for computing single-source
  shortest paths.}

\begin{verbatim}
procedure dijkstra(G, l, s)
input:   Graph G = (V, E), directed or undirected;
         positive edge length {l_e : e in E}; vertex s in V
output:  For all vertices u reachable from s, dist(u) is set
         to the distance from s to u

for all u in V:
    dist(u) = inf
    prev(u) = nil
dist(s) = 0;

H = makeque(V) (using dist-values as keys)
while H is not empty:
    u = deletemin(H)
    for all edges (u, v) in E:
        if (dist(v) > dist(u) + l(u, v))
        dist(v) = dist(u) + l(u, v)
        prev(v) = u
        decreaseKey(H, v)

djkstra(G, w, s) {
    initialize-single-source(G, s)
    s <- empty set
    q = V[G]
    while Q is not empty
        do u = extract-min(Q)
        S = S union {u}
        for each vertex v in Adj[u]
            do relax(u, v, w)
    }
\end{verbatim}
\item
  \textbf{What is the requirement in order to be able to apply
  Dijkstra's algorithm?}

  Edge weight must all be positive.
\item
  \textbf{You may be provided an example graph and asked to return the
  search tree that arises from Dijkstra, as well as the state of the
  priority queue during its iteration of the algorithm.}

  Can do.
\item
  \textbf{Prove the correctness of Dijkstra's algorithm on non-negative
  weight graphs.}

  Cannot do.
\item
  \textbf{What is the best running time of Dijkstra's algorithm and for
  what implementation of the priority queue data structure? What is the
  running time of Dijkstra's algorithm if the priority queue is
  implemented as a simple array? What is the running time of Dijkstra's
  algorithm if the priority queue is implemented as a binary heap?}

  \begin{longtable}[c]{@{}llll@{}}
  \toprule\addlinespace
  Implementation & \texttt{deletemin} & \texttt{increase/decreasekey} &
  Total
  \\\addlinespace
  \midrule\endhead
  Array & $O(|V|)$ & $O(1)$ & $O(|V|^2)$
  \\\addlinespace
  Binary heap & $O(\log|V|)$ & $O(\log|V|)$ & $O((|V| + |E|) \log|V|)$
  \\\addlinespace
  $d$-ary hear & $O(\frac{d \log |V|}{\log d})$ &
  $O(\frac{\log |V|}{\log d})$ &
  $O((|V| \times d + |E|) \frac{\log|V|}{\log d})$
  \\\addlinespace
  Fibonacci heap & $O(\log |V|)$ & $O(1)$ & $O(|V|\log |V| + |E|)$
  \\\addlinespace
  \bottomrule
  \end{longtable}
\item
  \textbf{When is each of these implementations preferred over the
  other?}

  It depends whether the the graph is \emph{dense} or \emph{sparse}
  edges. To be dense is to have many edges, to be sparse is to have
  fewer edges. For all graphs, $|E|$ is less than $|V|^2$. If it is
  $\Omega(|V|^2)$, then cearly the array implementation is faster. On
  the other hand, the binary heap becomes preferable as soon as $|E|$
  dips below \textbar{}$|V|^2 \log |V|$.
\end{enumerate}

\subsection{Bellman-Ford and Floyd-Warshall
algorithms}\label{bellman-ford-and-floyd-warshall-algorithms}

\subsubsection{Chapter 4.6 from DPV: Shortest paths in the presence of
negative
edges}\label{chapter-4.6-from-dpv-shortest-paths-in-the-presence-of-negative-edges}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  How can you account for the fact that when you have a negative edge,
  the shortest path may go through more nodes than any other path?
\item
  What has to change to accomodate this sort of change?

  \begin{itemize}
  \item
    Distance estimates under dijkstra are always overestimates or
    exactly correct. Consider:

\begin{verbatim}
procedure update((u, v) in E) {
    dist(v) = min(dist(v), dist(u) + l(u, v));
}
\end{verbatim}
  \item
    This \emph{update} operation is simply an expression of the fact
    that the distance to $v$ cannot possibly be more than the distance
    to $u$. It has the following properties:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      It gives the corect distance to $v$ in the case where it is the
      second last node in the shortest path to $v$.
    \item
      It will never make \texttt{dist(v)} too small, making it
      \emph{safe}.
    \end{enumerate}
  \end{itemize}
\end{itemize}

\paragraph{Negative cycles}\label{negative-cycles}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  In situations with negative edge weights, it doesn't make sense to
  even ask about shortest paths.
\item
  The shortest-path problem is ill-posed in graphs with negative cycles.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Shortest path slipped in when we used the notion of ``existence''
    (evidently).
  \end{itemize}
\item
  There is a negative cycle if and only if some \texttt{dist} value is
  reduced during the final round.
\end{itemize}

\subsubsection{Chapter 4.7 from DPV: Shortest paths in
dags}\label{chapter-4.7-from-dpv-shortest-paths-in-dags}

\begin{itemize}
\item
  There are two subclasses of graph that exclude the possibility of
  negative cycles:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Graphs without negative edges
  \item
    Graphs without cycles
  \end{enumerate}
\item
  The key source of efficiency:

  \begin{quote}
  In any path of a dag, the vertices appear in increasing linearized
  order.
  \end{quote}
\item
  You can find the longest path in a dag by negating all edge lengths.
\end{itemize}

\subsubsection{Chapter 6.1 from DPV: Shortest paths in dags,
revisited}\label{chapter-6.1-from-dpv-shortest-paths-in-dags-revisited}

\begin{itemize}
\item
  The special feature of dags is that the nodes can be
  \emph{linearized}, that is, put on a straight line from left to right.
\item
  You can compute all distances with a single pass of the following
  algorithm:

\begin{verbatim}
initialize all dist(.) values to inf;
dist(s) = 0;
for each v in V\{s}, in linearized order:
    dist(v) = min_{(u, v) in E} (dist(u) + L(u, v));
\end{verbatim}
\item
  Dynamic programming is a very powerful algorithmic paradigm in which a
  problem is solved by indentifying a collection of subproblem and
  tacking them one by one, smallest first, using the answers to small
  problems to help figure out larger onces, until the whole lot of them
  are solved.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    In dynamic programming, you aren't always given a dag, the dag is
    implicit.
  \item
    It's nodes are the subproblems we define, and it edges are the
    dependancies between subproblems:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      If we solve subproblem $B$ we need to answer subproblem $A$,
      making a conceptual edge from $A$ to $B$.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 24.1 from CLRS2: The Bellman-Ford
Algorithm}\label{chapter-24.1-from-clrs2-the-bellman-ford-algorithm}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The Bell-Ford Algorithm solves the single-source shortest-path in the
  general case in which edge weight may be negative.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Given a weight, directed graph $G = (V, E)$ with source $s$ and
    weight function $w : E \to R$, the Bellman-Ford algorithm returns a
    boolean value indicating whether or not there is a negative weight
    cycle that is reachable from the source.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      If there is such a cycle, the algorithm indicates that there is no
      such solution.
    \end{itemize}
  \item
    If there is no cycle, the algorithm produces the shortest paths and
    their weights.
  \end{itemize}
\item
  The algorithm uses relaxation, progessively decreasing an estimate
  $d[v]$ on the weight of a shortest path.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The algorithm returns true if and only if there are no negative
    weighted cycles that are reachable form the source.
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 24.2 from CLRS2: Single-source shortest paths in
directed acyclic
graphs}\label{chapter-24.2-from-clrs2-single-source-shortest-paths-in-directed-acyclic-graphs}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  By relaxing the edges of a weighted dag according to a toplogical sort
  of vertices, we can compute shortest paths from a single source in
  $\Omega(V + E)$ time.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Shortest paths are always well-edfined in a dag, since even if there
    are non-negative edge weights, there are no negative cycles (because
    there are no cycles).
  \end{itemize}
\item
  The algorithm starts by topologically sorting the dag to impose a
  linear ordering on the vertices. If there is a path from $u$ to $v$,
  it is topologically latter.

  \begin{itemize}
  \item
    Only need a single pass.

    dag-shortest-paths(G, w, s) \{ topologically sort the vertices of G
    initialize-single-source(G, s); for each vertex U, take in
    topologically sorted order do for each vertex v in Adj{[}u{]} do
    relax(u, v, w); \}
  \end{itemize}
\item
  The running tim eof this is easy.

  \begin{itemize}
  \item
    The topological sort is \[\Omega(V + E)\]
  \item
    The initialize-single-source takes \[\Omega(V)\]
  \item
    Then to two inner loops examine every $V$ and $E$. \[\Omega(V + E)\]
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Provide the Bellman-Ford algorithm for computing single-source
  shortest paths.}

\begin{verbatim}
procedure shortest-paths(G, l, s) { 
input:  Directed graph G = (V, E);
        edge lengths {l_e : e in E} with no negative cycles;
        vertex s in V
output: For all vertices u reachable form s, dist(u) is set
        to the distance from s to u.

for all u in V:
    dist(u) = inf;
    prev(u) = nil;

dist(s) = 0;
repeat|V| - 1 times:
    for all e in E:
        update(e);
}

procedure BellmanFord(list vertices, list edges, vertex source) {
    // step 1: initialization
    for each vertex v in vertices:
        if v is source then distace[v] := 0;
        else distace[v] := infinity
        precessir[v] := null

    // step 2: relax edges repeatedly
    for i from 1 to size(vertices) - 1:
        for each edge (u, v) with weight w in edges:
            if distace[u] + v < distance[v]:
            distance[v] := distance [u] + w
            predecessor[v] := u

    // step 3: check for negative weight cycles
    for each edge (u, v) with weight w in edges:
        if distance[u] + w < distance[v]:
            error "Graph containts negative weight cycle."
}
\end{verbatim}
\item
  \textbf{What is the running time of the approach and why?}

  The Bellman-Ford algorithm run in time $O(V E)$ since the
  initialization takes $O(V)$ time, and each of the $|V| - 1$ passes
  over the edges takes $\Theta(E)$ time, and the for look takes $O(E)$
  time.
\item
  \textbf{Why is it correct?}
\item
  \textbf{You may be provided a graph and asked to trace the dynamic
  programming matrix that arises from the operation of Bellman-Ford.}
\item
  \textbf{How can you detect the existence of negative cycles using the
  Bellman-Ford algorithm?}

  There is a look at the end like so,

\begin{verbatim}
 for each edge (u, v) with weight w in edges:
    if distance[u] + w < distance[v]:
        error "Graph containts negative weight cycle."
\end{verbatim}
\item
  \textbf{What is an efficient approach for computing single-source
  shortest paths on directed acyclic graphs that may contain negative
  weight edges and what is the running time of this solution?}
\item
  \textbf{Why is the Floyd-Warshall algorithm preferred over calling
  \textbar{}V\textbar{} times the Bellman-Ford algorithm on a graph to
  solve all-pair shortest path problems?}
\item
  \textbf{What is the dynamic programming formulation of Floyd-Warshall,
  i.e., what are the subproblems defined by the algorithm and how are
  they combined in order to address more complex problems?}
\end{enumerate}

\subsection{Floyd-Warshall and Johnson's
algorithms}\label{floyd-warshall-and-johnsons-algorithms}

\subsubsection{Chapter 25.2 from CLRS2: The Floyd-Warshall
algorithm}\label{chapter-25.2-from-clrs2-the-floyd-warshall-algorithm}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A dynamic-programming formulation to solve the all-pairs
  shortest-paths problem on a directed graph $G = (V, E)$.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Floyd-warshall runs in $\Omega(V^3)$ time.
  \item
    Negative weight edges may be present, but we assume that there are
    no negative-weight cycles.
  \item
    Will follow the dynamic-programming process to develop the algorith,
  \end{itemize}
\end{itemize}

\paragraph{The structure of a shortest
path}\label{the-structure-of-a-shortest-path}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The FW algorithm uses a different charaterization of the shortest path
  than the matrix-multiplcation-based all-pairs algorithms.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The algorithm considers the ``intermediate'' vertices of a shortest
    path, where intermediate means a node on the shortest path.
  \end{itemize}
\item
  Assume that the vertices of $G$ are
  $V = \lbrace 1, 2, \cdots, n \rbrace$ and consider the ubset
  $\lbrace 1, 2, \cdots, k \rbrace$ of vertices for some $k$.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The Floyd-Warshall algorithm explain a relationship between path $p$
    and shortest paths from $i$ to $j$ with all intermediate nodes in
    $1, 2, \cdots, k - 1 \rbrace$.
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 25.3 from CLRS2: Johnson's algorithm for sparse
graphs}\label{chapter-25.3-from-clrs2-johnsons-algorithm-for-sparse-graphs}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Johnson's algorithm finds shortest paths between all pairs in
  $O(V^2 \log V + VE)$ time.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    For sparese graphs, it is asymptotically better than either repeated
    squaring matrices of the Floyd-Warshall algorithm.
  \item
    The algorithm either returns a matrix of short-path weights for all
    pairs of matrices or reports that the graph contains a negative
    weight cycle.
  \end{itemize}
\item
  Johnson's algorithm uses the technique of \emph{reweighting}, which
  is:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    If all edge weights $w$ in graph $G = (V, E)$ are nonnegative, we
    can find shortest paths between all pairs of certices by running
    Dijkstra's algorithm once from each vertex.
  \item
    With the Fibonacci-heap min-priority queue, the running time of this
    all-pairs will be $O(V^2 \log V + VE)$.
  \item
    If $G$ has negative-weight edges but no negative-weight cycles, we
    simply compute a new set of nonnegative edge weights that allow us
    to use the same method.

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      The new set of edge weights must satisfy two important properties:

      \begin{enumerate}
      \def\labelenumi{\arabic{enumi}.}
      \itemsep1pt\parskip0pt\parsep0pt
      \item
        For all pairs of vertices $u$, $v \in V$, a path $p$ is a
        shortest path from $u$ to $v$ using weight function $w$ if and
        only if $p$ is also a shortest path from $u$ to $v$ using weight
        function $w*$
      \item
        For all edges $(u, v)$, the new weight $w * (u, v)$ is
        nonnegative.
      \end{enumerate}
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 6.6 from DPV2}\label{chapter-6.6-from-dpv2}

\subsubsection{Practice questions}\label{practice-questions-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Provide the Floyd-Warshall algorithm for solving all-pair
  shortest path problems. What is the running time of the approach?}

\begin{verbatim}
floud-warshall() {
    let dist be a |V| x |V| array of minimum distances initialized to inf
    for each vertex v:
        dist[v][v] = 0;

    for each edge (u, v) {
        dist[u][v] := w(u, v); // the weight of the edge
    }

    for k from 1 to |V|
        for i from 1 to |V|
            for j from 1 to |V|
                if dist[i][j] > dist[i][k] + dist[k][j]
                    dist[i][j] = dist[i][k] + dist[k][j]
}
\end{verbatim}
\item
  \textbf{You may be provided a graph and asked to compute the dynamic
  programming matrix that arises from a few iterations of the
  Floyd-Warshall algorithm.}
\item
  \textbf{What is the transitive closure of a graph and how can it be
  computed following a dynamic programming approach? What is the
  relation to the Floyd-Warshall algorithm?}

  The transitive closure of $G$ is defined as the graph
  $G * = (V, E *)$, where
  $E* = \lbrace(i, j) : \text{there is a path from vertex} i \text{to vertuex} j \text{in} G \rbrace$.
\item
  \textbf{If a graph has non-negative edges, is it preferable to run the
  Floyd-Warshall algorithm to solve an all-pair shortest path problem or
  is it preferable to call \textbar{}V\textbar{} times Dijkstra's
  algorithm?}

  The complexity for running Dijkstra on all nodes will be
  $O(EV + V^2 \log V)$. This complexity is lower than $O(V^3)$ iff
  $E < V^2$.
\item
  \textbf{What is the main idea in Johnson's algorithm and why can it be
  preferable over the Floyd-Warshall algorithm when solving all-pair
  shortest path problems?}

  The main idea behind Johnson's algorithm is that it uses Bellman-Ford
  to remove all negative edge weights from a graph using an artificial
  vertex, and then performs Djikstra's algorithm to solve the all-pair
  shortest path problem.

  It will be preferable over Floyd-Warshall because it has a better run
  time.
\item
  \textbf{Given the above transformation of weights for a graph G(V,E),
  how should the values h be computed for the vertices of the graph so
  that all weights end up having non-negative values?}
\item
  \textbf{Describe Johnson's algorithmic steps. What is its running
  time?}

\begin{verbatim}
Johnson(G) {
    compute G', where V[G'] = V[G] union {s},
        E[G'] = E[G] union {(s, v) : v in V[G]} and
        w(s, v) = 0 for all v in V[G]

    if Bellman-Ford(G', w, s) = false
        then print "the input graph contain a negative-weight cycle"
        else for each vertex v in V[G']
            do set h(v) to the value of delta(s, v)
                computed by Bellman-Ford

        for each edge (u, v) in E[G']
            do w*(u, v) := w(u, v) + h(u) - h(v);
        for each vertex u in V[G]
            do run Dikstra(G, w*, u) to compute delta *(u, v) for all v in V[G]
                for each vertex v in V[G]
                    do d_{u, v} = delta *(u, v) + h(v) - h(u)

        return D
}
\end{verbatim}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The steps to the algorithm:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      A new node $q$ is added to the graph, connected by zero-weight
      edges to each of the other nodes.
    \item
      The Bellman-Ford algorithm is used, starting from the new vertex
      $q$ to find for each vertex $v$ the minimum height $h(v)$ of a
      path from $q$ to $v$. If this step detects a negative cycle, the
      algorithm is terminated.
    \item
      The edges of the original graph are reweighted using the value
      computed by the Bellman-Ford algorithm: an edge from $u$ to $v$,
      having length $w(u, v)$, is given the new length
      $w(u, v) + h(u) - h(v)$.
    \item
      $q$ is removed, and Dijkstra's algorithm is used to find the
      shortest paths from each node $s$ to every other vertex in the
      reweighted graph.
    \end{enumerate}
  \item
    Time complexity:

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      Using Fibonacci heaps in the implementatino of Dijkstra's
      algorithm is $O(V^2 \log V + VE)$.

      \begin{itemize}
      \itemsep1pt\parskip0pt\parsep0pt
      \item
        The algorithm uses $O(VE)$ time for the Bellman-Ford stage of
        the algorith,
      \end{itemize}
    \item
      $O(V \log V + E)$ for each of the $V$ instantiations of Dijkstra's
      algorithm.
    \item
      Thus, when the graph is sparse, the total time will be faster than
      the Floyd-Warhshall algorithm, which solves the same problem in
      $O(V^3)$.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\section{Final Study Guide}\label{final-study-guide}

\subsection{Minimum Spanning Trees}\label{minimum-spanning-trees}

\subsubsection{Chapter 5.1 from DPV: Minimum spanning
trees}\label{chapter-5.1-from-dpv-minimum-spanning-trees}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  This is in search of the optimal set of edges.
\item
  This cannot contain an edge -- edges are always extraneous.
\end{itemize}

\paragraph{A greedy approach}\label{a-greedy-approach}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Kruskal's minimum spanning tree algorithm starts with an empty graph
  and then selects edges from $E$ according to the following rule:
\end{itemize}

\begin{quote}
Repetedly add the next lightest edge that doesn't produce a cycle.
\end{quote}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  It constructs the tree edge by edge, and simply picks whichever edge
  is cheapest at the moment.
\end{itemize}

\paragraph{Cut property}\label{cut-property}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Cut property]
Suppose edges $X$ are part of a minimum spanning tree of $G = (V, E)$.
Pick any subset of nodes $S$ for which $X$ does not cross between $S$
and $V - S$, and let $e$ be the lightest edge across this partition.
Then $X \bigcup \lbrace e \rbrace$ is a part of some MST.
\end{description}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A \emph{cut} is any partition of the vertices into two groups, $S$ and
  $V - S$. What this property says is that it is always safe to add the
  lightest edge across any cut provided $X$ has no edges across the cut.
\end{itemize}

\[T' = T \cup \lbrace e \rbrace - \lbrace e' \rbrace\]

\paragraph{Kruskal's algorithms}\label{kruskals-algorithms}

At any given moment, the edge it has already chose form a partial
solution, a collection of connected components each of which has a tree
structure. The next edge $e$ to be added connects two of these
components; call them $T_1$ and $T_2$. Since $e$ is the lightest edge
that doesn't produce a cycle, it is certain to be the lightest edge
between $T_1$ and $V - T_1$ and therefore satisfyies the cut property.

Now we fill in some implementation details. At each state, the algorithm
chooses an edge to add to its current partial solution. To do so, it
need to test each candidate edge $u - v$ to see whether the endpoints
$u$ and $v$ lie in different components; otherwise the edge produces a
cycle. And once an edge is chose, the corresponding components need to
be merged. What kind of data structure supports this operation?

We will model the algorithm's state as a collection of disjoint sets,
each of which contains the nodes of a particular component.

\texttt{makeset(x)}: create a singleton set containing just $x$.
Initially each node is in a component by itself.

\texttt{find(x)}: to which set does $x$ belong? We repeatedly test pairs
of nodes to see if they belong to the same set.

\texttt{union(x, y)}: merge the sets containing $x$ and $y$. And
wherever we add an edge, we are merging two components.

\begin{verbatim}
procedure kruskal(G, w)
input:  a connect undirect graph G = (V, E) wieth edge weights w_e.
output: a minimum spanning tree defined by the edges X

for all u in V
    makeset(u)

X = {}
Sort the edges E by weight
for all edges {u, v} in E, in increasing order of weight:
    if find(u) != find(v)
        add edge {u, v} to X
        union(u, v)
\end{verbatim}

\subsubsection{Chapters 23.1 from CLRS2: Growing a minimum spanning
tree}\label{chapters-23.1-from-clrs2-growing-a-minimum-spanning-tree}

\begin{verbatim}
generic-MST(G, w)
    a = {}
    while A does not form a spanning tree
        do find an edge (u, v) that is safe for A
            A = A + {(u, v)}
    return A
\end{verbatim}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A \emph{cut} $(S, V - S)$ of an undirected graph $G = (V, E)$ is a
  partition of $V$.
\item
  We say that an edge $(u, v) \in E$ \emph{crosses} the cut $(S, V - S)$
  if one of its endpoints is in $S$ and the other is in $V - S$
\item
  We say that a cut \emph{respects} a set $A$ of edge if no edge in $A$
  crosses the cut.
\item
  An edge is a \emph{light edge} crossing a cut if its weight is the
  minimum of any edge crossing the cut.
\item
  More generally, we say an edge is a \emph{light edge} satisfying a
  given property if its weight is the minimum of any edge satisfying the
  property.
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What kind of graph-based optimization problems require as a
  solution the computation of a Minimum Spanning Tree?}

  In the design of electronic circuitry, it is often necessary to make
  the pins of several components electrically equivalent by wiring them
  together. To interconnect a set of $n$ pins, we can use an arrangement
  of $n - 1$ wires, each connecting two pins. Of all such arrangements,
  the one that uses the least amount of wire is usually the most
  desirable.
\item
  \textbf{Why the solution to such problems does not contain cycles?}

  The solution to any MST problem does not contain cycles because of the
  property that any cycle can be removed and disconnect a graph -- they
  are always extraneous.
\item
  \textbf{Provide a generic description of an algorithm for the
  computation of minimum spanning trees.}
\item
  \textbf{What is a cut of a graph?}

  A cut is a division of vertices in an MST into $S$ and $S - V$. The
  cut property says that it is always safe to add the lightest edge
  between the two groups and get an MST when the extraneous edge is
  removed.
\item
  \textbf{When does an edge cross a cut?}

  When this is true, where $T'$ and $T$ are MSTs:

  \[T' = T \cup {e} - {e'}\]
\item
  \textbf{When does a cut respect a set of edges?}

  We say that a cut \emph{respects} a set $A$ of edge if no edge in $A$
  crosses the cut.
\item
  \textbf{What is the definition of a light edge?}

  An edge is a \emph{light edge} crossing a cut if its weight is the
  minimum of any edge crossing the cut. More generally, we say an edge
  is a \emph{light edge} satisfying a given property if its weight is
  the minimum of any edge satisfying the property.
\item
  \textbf{Assume a connected, undirected graph $G(V, E)$ with weights
  $w$ and a set of edges $A \in E$ that is a subset of minimum spanning
  tree $T$. Consider then any cut $(S, V - S)$ that respects the set
  $A$. Prove that if $(u, v)$ is a light-edge crossing $(S, V - S)$,
  then $(u, v)$ can be safely be added to A as an edge that is part of a
  minimum spanning tree of G.}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Let $T$ be a minimum spanning tree that includes $A$, and assume
    that $T$ does not contain the light edge $(u,v)$, since if it does,
    we are done. We shall construct another minimum spanning tree $T'$
    that includes $A \cup \lbrace(u, v)\rbrace$ by using a cut-and-paste
    technique, thereby showing that $(u, v)$ is a safe edge for $A$.
  \item
    The edge $(u,v)$ forms a cycle with the edges on the path $p$ from
    $u$ to $v$ in $T$. Since $u$ and $v$ are on opposite sides of the
    cut $(S, V - S)$, there is at least one edge in $T$ on the path $p$
    that also crosses the cut. Let $(x, y)$ be any such edge. The edge
    $(x, y)$ is not in $A$, because the cut respects $A$. Since $(x, y)$
    is on the unique path from $u$ to $v$ in $T$, removing $(x, y)$
    breaks $T$ into two components. Adding $(u,v)$ reconnects them to
    form a new spanning tree
    $T = T - \lbrace (x,y) \rbrace \cup \lbrace (u,v) \rbrace$.
  \item
    We next show that $T$ is a minimum spanning tree. Since $(u, v) $ is
    a light edge crossing $(S, V - S)$ and $(x, y)$ also crosses this
    cut, $w(u, v) \le w(x, y)$. Therefore,
  \end{itemize}

  \[w(T') = w(T) - w(x, y) + w(u,v)\] \[\le w(T)\]

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    But $T$ is a minimum spanning tree, so that $w(T) \le w(T')$; thus,
    $T'$ must be a minimum spanning tree also.
  \end{itemize}
\item
  \textbf{Given the above statement, prove the following: Let
  $C = (V_C,E_C) $ be a connected component (tree) in the forest
  $GA(V,A)$. If $(u, v)$ is a light-edge connecting $C$ to another
  component in $G_A$, then $(u, v)$ is safe for $A$.}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The cut $(V_C,V - V_C)$ respects $A$, and $(u,v)$ is a light edge
    for this cut. Therefore, $(u, v)$ is safe for $A$.
  \end{itemize}
\end{enumerate}

\subsection{Kruskal's and Prim's
algorithms}\label{kruskals-and-prims-algorithms}

\subsubsection{Chapters 23.2 from CLRS2: The algorithms of Kruskal and
Prim}\label{chapters-23.2-from-clrs2-the-algorithms-of-kruskal-and-prim}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Each algorithm given here is a filling in of the generic MST.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    In Kruskals, the set $A$ is a forest. The safe edge added to $A$ is
    always a least-weight edge connecting the tree to a vertex not in
    the tree.
  \item
    In Prim's, the set $A$ forms a single tree. The sage edge added to
    $A$ is always a least-weight edge connecting the tree to a vertex in
    the tree.
  \end{itemize}
\end{itemize}

\paragraph{Kruskal's}\label{kruskals}

\subparagraph{Description}\label{description}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  It finds a safe edge to add to the growing forest by finding, of all
  the edges that connect any two trees in the forest, an edge $(u,v)$ of
  least weight.
\item
  Kruskal's algorithm is a greedy algorithm, because at each step it
  adds to the forest an edge of least possible weight.
\item
  It uses a disjoint-set data structure to maintain several disjoint
  sets of elements.
\item
  Each set contains the vertices in a tree of the current forest. The
  operation FIND-SET(u) returns a representative element from the set
  that contains u.
\item
  Thus, we can determine whether two vertices u and v belong to the same
  tree by testing whether FIND-SET(u) equals FIND-SET(v).
\item
  The combining of trees is accomplished by the UNION procedure.
\end{itemize}

\subparagraph{Steps}\label{steps}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Lines 1--3 initialize the set A to the empty set and create
  \textbar{}V \textbar{} trees, one containing each vertex.
\item
  The edges in E are sorted into nondecreasing order by weight in line
  4.
\item
  The for loop in lines 5--8 checks, for each edge (u,v), whether the
  endpoints u and v belong to the same tree.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    If they do, then the edge (u, v) cannot be added to the forest
    without creating a cycle, and the edge is discarded.
  \item
    Otherwise, the two vertices belong to different trees.
  \end{itemize}
\end{itemize}

\subparagraph{Running time}\label{running-time}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  with the union-by-rank and path-compression heuristics, since it is
  the asymptotically fastest implementation known.
\item
  Initializing the set $A$ in line 1 takes $O(1)$ time,
\item
  and the time to sort the edges in line 4 is $O(E \lg E)$.
\item
  The for loop of lines 5--8 performs $O(E)$ \texttt{FIND-SET} and
  \texttt{UNION} operations on the disjoint-set forest. Along with the
  $|V|$ \texttt{MAKE-SET} operations, these take a total of
  $O((V + E) \alpha (V))$ time,
\item
  $\alpha (|V|) = O(\lg V) = O(lg E)$, the total running time of
  Kruskal's algorithm is $O(E \lg E)$. Observing that $|E| < |V|^2$, we
  have $\lg |E| = O(\lg V )$, and so we can restate the running time of
  Kruskal's algorithm as $O(E \lg V )$.
\end{itemize}

\paragraph{Prim's}\label{prims}

\subparagraph{Description}\label{description-1}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  At each step of the algorithm, the vertices in the tree determine a
  cut of the graph, and a light edge crossing the cut is added to the
  tree.
\item
  In the second step, for example, the algorithm has a choice of adding
  either edge $(b, c)$ or edge $(a, h)$ to the tree since both are light
  edges crossing the cut.
\end{itemize}

\subparagraph{Steps}\label{steps-1}

\begin{itemize}
\item
  Lines 1--5 set the key of each vertex to infinity (except for the root
  $r$, whose key is set to $0$ so that it will be the first vertex
  processed), set the parent of each vertex to \texttt{NIL}, and
  initialize the min- priority queue $Q$ to contain all the vertices.
  The algorithm maintains the following three-part loop invariant:
\item
  Prior to each iteration of the while loop of lines 6--11,

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    $A=\lbrace (v, \pi [v]) v \in V- \lbrace r\rbrace - Q \rbrace$.
  \item
    The vertices already placed into the minimum spanning tree are those
    in $V - Q$.
  \item
    For all vertices $v \in Q$, if $\pi [v] \not = NIL$, then
    $key[v] < \infty$ and \% is the weight of a light edge $(v,\pi [v])$
    connecting $v$ to some vertex already placed into the minimum
    spanning tree.
  \end{enumerate}
\item
  Line 7 identifies a vertex $u \in Q$ incident on a light edge crossing
  the cut $(V - Q, Q)$ (with the exception of the first iteration, in
  which u = r due to line 4). Removing $u$ from the set $Q$ adds it to
  the set $V - Q$ of vertices in the tree, thus adding $(u, \pi [u])$ to
  $A$. The for loop of lines 8--11 update the key and $\pi$ fields of
  every vertex $v$ adjacent to $u$ but not in the tree. The updating
  maintains the third part of the loop invariant.
\end{itemize}

\subparagraph{Running time}\label{running-time-1}

\subsubsection{Chapter 21.2 from CLRS2: Linked-list representation of
disjoint
sets}\label{chapter-21.2-from-clrs2-linked-list-representation-of-disjoint-sets}

\begin{itemize}
\item
  A simple way to implement a disjoint-set data structure is to
  represent each set by a linked list. The first object in each link
  list serves as its set's representative. Each object in the linked
  list contains a set member, a point to the object containing the next
  set member, and a pointer back to the representative. Each list mains
  pointers \emph{head} to the representative and \emph{tail} to the last
  object in the list. Within each linked list, the objects may appear in
  any order.
\item
  With linked-list representation, both \texttt{Make-Set} and
  \texttt{Find-Set} are easy, requiring $O(1)$ time. To carry out
  \texttt{Make-Set(x)}, we create a new linked list whose only object is
  $x$. For \texttt{Find-Set}, we just return the pointer from $x$ back
  to the representative.
\end{itemize}

\subsubsection{Chapter 21.3 from CLRS2: Disjoint-set
forests}\label{chapter-21.3-from-clrs2-disjoint-set-forests}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  In a faster implementation of disjoint sets, we represent sets by
  rooted trees, with each node containing one member and each tree
  representing one set.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    In a disjoint-set forest, illustrated in Figure 21.4(a), each member
    points only to its parent.
  \item
    The root of each tree contains the representative and is its own
    parent.
  \item
    As we shall see, although the straightforward algorithms that use
    this representation are no faster than ones that use the linked-list
    representation, by introducing two heuristics - union by rank and
    path compression - we can achieve the asymptotically fastest
    disjoint-set data structure known.
  \end{itemize}
\item
  We perform the three disjoint-set operations as follows.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    A MAKE-SET operation simply creates a tree with just one node.
  \item
    We perform a FIND-SET operation by following parent pointers until
    we find the root of the tree. The nodes visited on this path toward
    the root constitute the find path.
  \item
    A UNION operation, shown in Figure 21.4(b), causes the root of one
    tree to point to the root of the other.
  \end{itemize}
\item
  The first heuristic, \textbf{union by rank}, is similar to the
  weighted-union heuristic we used with the linked-list representation.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The idea is to make the root of the tree with fewer nodes point to
    the root of the tree with more nodes.
  \item
    Rather than explicitly keeping track of the size of the subtree
    rooted at each node, we shall use an approach that eases the
    analysis.
  \item
    For each node, we maintain a rank that is an upper bound on the
    height of the node.
  \item
    In union by rank, the root with smaller rank is made to point to the
    root with larger rank during a UNION operation.
  \end{itemize}
\item
  The second heuristic, \textbf{path compression}, is also quite simple
  and very effective.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    As shown in Figure 21.5, we use it during FIND-SET operations to
    make each node on the find path point directly to the root.
  \item
    Path compression does not change any ranks.
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Given the generic algorithm for the computation of a minimum
  spanning tree, how is the safe edge computed in Kruskal's algorithm?}

  It finds a safe edge to add to the growing forest by finding, of all
  the edges that connect any two trees in the forest, an edge $(u,v)$ of
  least weight.
\item
  \textbf{How is it computed in Prim's algorithm?}

  At each step, a light edge is added to the tree $A$ that connects $A$
  to an isolated vertex of $G_A = (V, A)$
\item
  \textbf{You may be provided a graph and asked to trace the steps of
  Kruskal's and Prim's algorithms for the computation of the minimum
  spanning tree of the graph.}

  Okay!
\item
  \textbf{Provide Kruskal's algorithm.}

\begin{verbatim}
MST-Kruskal(G, w) {
    A = {}
    for each vertex v in V[G]
        do Make-Set(v)
    sort the edges of E into nondecreasing order by weight w
    for each edge (u, v) in E, taken in nondecreasing order by weight
        do if Find-Set(u) != Find-Set(v)
            then A = A + {(u, v)}
                Union(u,v)
    return A
\end{verbatim}
\item
  \textbf{Consider an implementation of the disjoint sets data structure
  with the union-by-rank heuristic as part of the implementation of
  Kruskal's algorithm. What is the running time of Kruskal's algorithm
  in this case and why?}
\item
  \textbf{Consider an implementation of the disjoint sets data structure
  with the union-by-rank and the path compression heuristic and that the
  edge weights are upper bounded by the value $|E|$. What is the running
  time of Kruskal's algorithm in this case and why?}

  \begin{itemize}
  \item
    Instation takes $O(1)$, and the time to sort the edges is
    $O(E \lg E)$
  \item
    The $|V|$ Make Set calls take $O(V + E \alpha (V))$

    \[\alpha (V) = O(\lg V) = O(\lg E)\]

    \[O(E \lg V)\]
  \end{itemize}
\item
  \textbf{Describe the \texttt{makeset}, \texttt{find\_set} and
  \texttt{union} functions of the disjoint sets data structure given the
  union-by-rank heuristic.}

  \begin{itemize}
  \item
    \texttt{makeset(x)}: create a singleton set containing just $x$.
    Initially each node is in a component by itself.
  \item
    \texttt{find(x)}: to which set does $x$ belong? We repeatedly test
    pairs of nodes to see if they belong to the same set.
  \item
    \texttt{union(x, y)}: merge the sets containing $x$ and $y$. And
    wherever we add an edge, we are merging two components.
  \item
    The idea is to make the root of the tree with fewer nodes point to
    the root of the tree with more nodes. Rather than explicitly keeping
    track of the size of the subtree rooted at each node, we shall use
    an approach that eases the analysis. For each node, we maintain a
    rank that is an upper bound on the height of the node. In union by
    rank, the root with smaller rank is made to point to the root with
    larger rank during a UNION operation.
  \end{itemize}
\item
  \textbf{What is the running time of these operations?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \texttt{makeset(x)} \[O(1)\]
  \item
    \texttt{find(x)} \[O(1)\]
  \item
    \texttt{union(x, y)} \[O(n)\]
  \end{itemize}
\item
  \textbf{How does the \texttt{find\_set} function change when you also
  use the path compression heuristic?}

\begin{verbatim}
Find-Set(x) {
    if x != p[x]
        then p[x] = Find-Set(p[x])
    return p[x];
}
\end{verbatim}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    This amortized cost turns out to be just barely more than O(1), down
    from the earlier O(log n).
  \end{itemize}
\item
  \textbf{What is the new running time of the \texttt{find\_set} and
  \texttt{union} functions in this case?}

  \[O(1)\]

  \[O(n)\]
\item
  \textbf{Provide Prim's algorithm.}

\begin{verbatim}
MST-Prim(G, w, r)
    for ech u in V[G]
        do key[u] = inf
            pi[u] = nil
    key[r] = 0
    Q = V[G]
    while Q != {}
        do u = extract-min(Q)
            for each v in Adj[u]
                do if v in Q and w(u, v) < key(u)
                    then pi[v] = u
                        key[v] = w(w, v)
\end{verbatim}
\item
  \textbf{What is the running time of Prim's algorithm given an array
  implementation of a priority queue?}
\item
  \textbf{What is the running time of the algorithm given a binary heap
  implementation of a priority queue?}
\end{enumerate}

\subsection{The class of NP problems - Examples of NP complete
problems}\label{the-class-of-np-problems---examples-of-np-complete-problems}

\subsubsection{Chapter 8.1 from DPV: Search
Problems}\label{chapter-8.1-from-dpv-search-problems}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Satisfiability is a problem of great practical important.

  \begin{itemize}
  \item
    Here's an instance:

    \[(x \lor y \lor z)(x \lor \lnot y)(y \lor \lnot z)(z \lor \lnot x)(\lnot x \lor \lnot y \lor \lnot z)\]
  \end{itemize}
\item
  This is \emph{Boolean formula in conjunctive normal form} (CNF)

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    There are \emph{clauses}
  \item
    There are \emph{literals}
  \end{itemize}
\item
  Is there are solution to this last case?
\end{itemize}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Search problem]
A \emph{search problem} is specified by an algorithm $C$ that takes two
inputs, an instance $I$ and a proposed solution $S$, and runs in time
polynomial in $|I|$. We say $S$ is a solution to $I$ if and only if
$C(I, S) = true$.
\end{description}

\begin{itemize}
\item
  Yet, interestingly, there are two natural variants of SAT for which we
  do have good algo- rithms. If all clauses contain at most one positive
  literal, then the Boolean formula is called a Horn formula, and a
  satisfying truth assignment, if one exists, can be found by the greedy
  algorithm of Section 5.3.
\item
  Alternatively, if all clauses have only two literals, then graph
  theory comes into play, and SAT can be solved in linear time by
  finding the strongly connected components of a particular graph
  constructed from the instance

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    In fact, in Chapter 9, we'll see a different polynomial algorithm
    for this same special case, which is called 2SAT.
  \end{itemize}
\item
  On the other hand, if we are just a little more permissive and allow
  clauses to contain three literals, then the resulting problem, known
  as 3SAT (an example of which we saw earlier), once again becomes hard
  to solve!
\end{itemize}

\paragraph{The traveling salesman
problem}\label{the-traveling-salesman-problem}

\begin{itemize}
\item
  In the TSP, we are given $n$ vertices $1, ..., n$ all $n(n - 1)/2$
  distances between them, as well as a \emph{budget} $b$. We are asked
  to find a you, a cycle that passes through every vertex exactly once,
  of total cost $b$ or less -- or to report that no such tour exists.

  \[d_{p(1),p(2)} + d_{p(2),p(3)} + ... + d_{p(n),p(1)} \le b\]
\item
  Notice that this defines TSP as a search problem, where given an
  instance, find a tour within the budget (or report that no such one
  exists).

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Why address it like this when it actually a \emph{optimization
    problem}?
  \end{itemize}
\item
  Turning an optimization problem into a search a problem does not
  chance its difficulty at all, because the two version \emph{reduce to
  one another}. Any algorithm that solves the optimization TSP also
  readily solves the search problem: find the optimum tour and if it
  within budget, return it, otherwise return no solution.
\item
  Conversely, an algorithm for the search problem can also be used to
  solve the optimization problem.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    To see why, first suppose that we show how knew the \emph{cost} of
    the optimum tour; then we could find this tour by calling the
    algorithm for the search problem, using the optimum cost as the
    budget.
  \item
    Fine, but how do we find the optimum cost? Easy: binary search.
  \end{itemize}
\item
  There is subtlety here: Why do we have to introduce a budget? Isn't
  any optimization problem also a search problem in the sense that we
  are searching for a solution that has the property of being optimal?

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The catch is that the solution to a search problem should be easy to
    recognition, or as we it early, polynomial time checkable. Given a
    potential solution to the TSP, it is easy to check the properties
    ``is a tour'' and ``has total length less than or equal to budget''
  \item
    But how could you check that the property ``is optimal''?
  \end{itemize}
\item
  As with SAT, there are no known polynomial time algorithms for the
  TSP.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    In Section 6.6 we saw a faster, yet still exponential, dynamic
    programming algorithm.
  \end{itemize}
\end{itemize}

\paragraph{Euler and Rudrata}\label{euler-and-rudrata}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \emph{Euler}: When can a graph be drawn without lifting the pencil
  from paper?

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    If and only if:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      The graph is connected.
    \item
      Every vertex with the possible exception of two vertices has even
      degree. (Degree is number of connected neighbors)
    \end{enumerate}
  \item
    Can be solved in polynomial time.
  \end{itemize}
\item
  \emph{Rudrata}: Can one visit all the squares of the chessboard
  without repeating any square, in one long walk that ends at the
  starting esquire and at eachs yep makes a legal knight move?

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Indeed, define the RUDRATA PATH problem to be just like RUDRATA
    CYCLE, except that the goal is now to find a path that goes through
    each vertex exactly once.
  \end{itemize}
\end{itemize}

\paragraph{Cuts and bisections}\label{cuts-and-bisections}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A cut is a set of edges whose removal leaves a graph disconnected.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    It is of interest to find small cuts, and this \textbf{minimum cut}
    problem is given a graph and a budget $b$, to find a cut with at
    most $b$ edges.
  \end{itemize}
\item
  the \emph{BALANCED CUT} problem is this: given a graph with n vertices
  and a budget $b$, partition the vertices into two sets $S$ and $T$
  such that $|S|,|T| \ge n/3$ and such that there are at most $b$ edges
  between $S$ and $T$.
\end{itemize}

\subsubsection{Chapters 34.2 from DPV: Polynomial-time
verification}\label{chapters-34.2-from-dpv-polynomial-time-verification}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  We now look at algorithms that verify membership in languages. For
  instance, suppose you have a grammar of the decision problem PATH, we
  are also given a path $p$ from $u$ to $v$.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    We can \emph{easily} check whether the length of $p$ is at most $k$,
    and if so, we can view $p$ as a ``certification'' that the instance
    indeed belongs to PATH.
  \end{itemize}
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the satisfiability problem?}

  The satisfiability problem is the problem of determining if there
  exists an interpretation that satisfies a given Boolean formula
\item
  \textbf{What is the worst-case running time of the best known
  algorithm for this problem?}
\item
  \textbf{What is the running time of checking whether a candidate
  solution is truly solving the problem or not?}
\item
  \textbf{Are there versions of the general satisfiability problem that
  can be solved in polynomial time? Describe two of them.}
\item
  \textbf{What is the traveling salesman problem?}

  The travelling salesman problem (TSP) asks the following question:
  Given a list of cities and the distances between each pair of cities,
  what is the shortest possible route that visits each city exactly once
  and returns to the origin city? It is an NP-hard problem in
  combinatorial optimization, important in operations research and
  theoretical computer science.

  TSP can be modelled as an undirected weighted graph, such that cities
  are the graph's vertices, paths are the graph's edges, and a path's
  distance is the edge's length. It is a minimization problem starting
  and finishing at a specified vertex after having visited each other
  vertex exactly once. Often, the model is a complete graph (i.e.~each
  pair of vertices is connected by an edge). If no path exists between
  two cities, adding an arbitrarily long edge will complete the graph
  without affecting the optimal tour.
\item
  \textbf{What is a dynamic programming approach for the traveling
  salesman approach?}

  Denote the cities by $1, ..., n$, the salesman's hometown being $1$,
  and let $D = (d_{ij})$ be the matrix of intercity distances.

  The goal is to design a tour that starts and ends at $1$, includes all
  other cities exactly once, and has minimum total length.

  The brute force approach takes $O(n!)$, but dynamic programming is
  better.

  Here is the appropriate subproblem:

  \begin{quote}
  For a subset of cities $S \in \lbrace 1,2,...,n \rbrace$ that includes
  $1$, and $j \in S$, let $C(S,j)$ be the length of the shortest path
  visiting each node in $S$ exactly once, starting at $1$ and ending at
  $j$.
  \end{quote}

  Now lets express $C(S, j)$ in terms of smaller subproblems. We need to
  state at $1$ and end at $j$; what should we pick as the secon-to-last
  city? It has to be some $i \in S$, so the overall path lentth is the
  distance from $1$ to $i$, namely $C(S - \lbrace j \rbrace, i)$, plus
  the length of the final edge, $d_{ij}$. We must pick the best such
  $i$:

  \[C(S, j) = \text{min}_{i \in S : i \not = j} C(S - \lbrace j \rbrace, i) + d_{ij}\]

  The problems are ordered by $S$. Here's the code:

\begin{verbatim}
C({1}, 1) = 0
for s = 2 to n:
    for all subsets S in {1, 2, ..., n} of sizes s and containing 1:
        C(S, 1) = infinity
        for all j in S, j != 1:
            C(S, j) = min {C(s - {j}, i) + d_ij : i in S, i != j}
return min_j C({1, ..., n}, j) + d_j1
\end{verbatim}
\item
  \textbf{What is the running time of this approach?}

  There are at most $2^n \times n$ subproblems, and each one takes
  linear time to solve. The total running time is therefore:

  \[O(n^2 2^n)\]
\item
  \textbf{Show that any optimization problem can be reduced to a search
  problem.}
\item
  \textbf{Show that any search problem can be reduced to an optimization
  problem.}
\item
  \textbf{Why do we typically prefer to work with search problems when
  studying the computational complexity of algorithms?}
\item
  \textbf{What is an Eulerian tour?}

  In graph theory, an Eulerian trail (or Eulerian path) is a trail in a
  graph which visits every edge exactly once. Similarly, an Eulerian
  circuit or Eulerian cycle is an Eulerian trail which starts and ends
  on the same vertex. They were first discussed by Leonhard Euler while
  solving the famous Seven Bridges of Königsberg problem in 1736.
  Mathematically the problem can be stated like this:

  \begin{quote}
  Given the graph on the right, is it possible to construct a path (or a
  cycle, i.e.~a path starting and ending on the same vertex) which
  visits each edge exactly once?
  \end{quote}

  Euler proved that a necessary condition for the existence of Eulerian
  circuits is that all vertices in the graph have an even degree, and
  stated without proof that connected graphs with all vertices of even
  degree have an Eulerian circuit. The first complete proof of this
  latter claim was published posthumously in 1873 by Carl Hierholzer.

  Every edge once.
\item
  \textbf{When does a graph have an Eulerian tour?}

  An undirected graph has an Eulerian cycle if and only if every vertex
  has even degree, and all of its vertices with nonzero degree belong to
  a single connected component.

  If and only if (a) the graph is connected and (b) every vertex, with
  the possible exception of two vertices (the start and final vertices
  of the walk), has even degree.
\item
  \textbf{What is the Rudrata cycle/path problem?}

  Let us define the Rudrata cycle search problem to be the following:
  given a graph, find a cycle that visits each vertex exactly once -- or
  report that no such cycle exists.

  This problem is ominously reminiscent of the TSP, and indeed no
  polynomial algirhtm is known for it.

  a Hamiltonian path (or traceable path) is a path in an undirected or
  directed graph that visits each vertex exactly once.

  Every vertex once.
\item
  \textbf{Is there a polynomial time algorithm for this problem?}
\item
  \textbf{What is the independent set problem?}

  an independent set or stable set is a set of vertices in a graph, no
  two of which are adjacent. That is, it is a set I of vertices such
  that for every two vertices in I, there is no edge connecting the two.
  Equivalently, each edge in the graph has at most one endpoint in I.
  The size of an independent set is the number of vertices it contains.
  Independent sets have also been called internally stable sets.

  The independent set problem is the search for a subset of nodes which
  contains no edges between them.
\item
  \textbf{Describe a dynamic programming solution for computing an
  independent set on trees.}

  Start by rooting the tree at any node $r$. Now, each node defines a
  subtree -- the one hanging from it. This immediately suggests
  subproblems:

  \[I(u) = \text{size of largest independent set of subtree hanging from } U\]

  Our final goal is $I(r)$.

  Dynamic programming proceeds as always from smaller subproblems to
  larger ones, that is to say, bottom-up in the rooted tree. Suppose we
  know the largest independent sets for all subtrees below a certain
  node $u$; in other words, suppose we know $I(w)$ for all descendants
  $w$ of $u$. How can we compute $I(u)$? Lets split the computation into
  two cases: any independent set include $u$ or it doesn't.

  \[I(u) = \text{max} \left \lbrace 1 + \sum_{\text{granchildren w of u}} I(w), \sum_{\text{children w of u}} I(w)  \right \rbrace\]

  If the independent sets includes $u$, then we get one point for it,
  but we aren't allowed to include the children of $u$ -- therefore, we
  move on to the grandchildren. This is the first case in the formula.
  On the other hand, if we don't include $u$, then we don't get a point
  for it, but we can move on to its children.
\item
  \textbf{What is the running time of this solution?}

  The number of subproblems is exactly the number of vertices. With a
  little care, the running time can be made linear.

  \[O(|V| + |E|)\]
\item
  \textbf{Is there a polynomial time algorithm for this problem on
  general graphs?}
\item
  \textbf{What is the vertex cover problem?}

  A vertex cover (sometimes node cover) of a graph is a set of vertices
  such that each edge of the graph is incident to at least one vertex of
  the set.

  A vertex cover of a graph $G = (V,E)$ is a subset of vertices
  $S \subset V$ that includes at least one endpoint of every edge in
  $E$.
\item
  \textbf{What is the clique problem?}

  The clique problem refers to any of the problems related to finding
  particular complete subgraphs (``cliques'') in a graph, i.e., sets of
  elements where each pair of elements is connected.
\item
  \textbf{Are there polynomial time algorithms for these problems?}
\end{enumerate}

\subsection{Additional examples of NP complete problems -
Reductions}\label{additional-examples-of-np-complete-problems---reductions}

\subsubsection{Chapter 8.2 from DPV: NP-complete
problems}\label{chapter-8.2-from-dpv-np-complete-problems}

\paragraph{Hard problems, NP Complete}\label{hard-problems-np-complete}

\begin{description}
\item[3SAT]
Like the satisfiability problem for arbitrary formulas, determining the
satisfiability of a formula in conjunctive normal form where each clause
is limited to at most three literals is NP-complete also; this problem
is called 3-SAT, 3CNFSAT, or 3-satisfiability.
\item[Traveling Salesman]
The travelling salesman problem (TSP) asks the following question: Given
a list of cities and the distances between each pair of cities, what is
the shortest possible route that visits each city exactly once and
returns to the origin city?
\item[Longest path]
the longest path problem is the problem of finding a simple path of
maximum length in a given graph.
\item[3D matching]
a 3-dimensional matching is a generalization of bipartite matching
(a.k.a. 2-dimensional matching) to 3-uniform hypergraphs.
\item[Knapsack]
Given a set of items, each with a mass and a value, determine the number
of each item to include in a collection so that the total weight is less
than or equal to a given limit and the total value is as large as
possible.
\item[Independent set]
an independent set or stable set is a set of vertices in a graph, no two
of which are adjacent.
\item[Integer linear programming]
a mathematical optimization or feasibility program in which some or all
of the variables are restricted to be integers.
\item[Rudrata path]
a Hamiltonian path (or traceable path) is a path in an undirected or
directed graph that visits each vertex exactly once.
\item[Balanced cut]
given a graph with $n$ vertices and a budget $b$, partition the vertices
into two sets $S$ and $T$ such that $|S|,|T| \le n/3$ and such that
there are at most $b$ edges between $S$ and $T$.
\end{description}

\paragraph{Easy problems (in P)}\label{easy-problems-in-p}

\begin{description}
\item[2SAT]
the problem of determining whether a collection of two-valued (Boolean
or binary) variables with constraints on pairs of variables can be
assigned values satisfying all the constraints.
\item[Minimum spanning tree]
Given a connected, undirected graph, a spanning tree of that graph is a
subgraph that is a tree and connects all the vertices together
\item[Shortest path]
problem of finding a path between two vertices (or nodes) in a graph
such that the sum of the weights of its constituent edges is minimized.
\item[Bipartite matching]
a matching or independent edge set in a graph is a set of edges without
common vertices.
\end{description}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Unary knapsack
\item
  Independent set on trees
\item
  Linear programming
\item
  Euler path
\end{itemize}

\begin{description}
\itemsep1pt\parskip0pt\parsep0pt
\item[Minimum cut]
a minimum cut of a graph is a cut (a partition of the vertices of a
graph into two disjoint subsets that are joined by at least one edge)
whose cut set has the smallest number of edges (unweighted case) or
smallest sum of weights possible.
\end{description}

The problems in P for a variety of reasons: dynamic programming, network
flow, graph search, greedy, etc.

The problems in NP are hard for the same reason! At their core, they are
all the same problem, but just different disguises. They are
\emph{equivilent}: as we shall see in 8.3, they can be reduced to one
another and back.

\paragraph{P and NP}\label{p-and-np}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  We know what a search problem is: it's defining characteristic is that
  any proposed solution can be quickly check for correctness, in the
  sense that there is an efficient checking algorithm $C$ that takes as
  input the given instance $I$ and outputs \texttt{true} if and only if
  $S$ really is a solution to instance $I$.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Moreover, the running tim of $C(I, S)$ is bounded by a polynomial in
    $|I|$, the length of the instnace.
  \item
    We denote the class of all search problems by \textbf{NP}.
  \end{itemize}
\item
  We've seen many examples NP search problems that are solvable in
  polynomial time.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    There is an algorithm that takes as input an instance $I$ and has a
    running time polynomial in $|I|$.
  \item
    If $I$ has a solution, the algorithm returns such a solution: and if
    $I$ has no solution, the alogirhtm correctly reports so.
  \item
    The class of all search problems that can be solved in polynomial
    time is denote by $P$.
  \end{itemize}
\end{itemize}

\paragraph{Reductions, again}\label{reductions-again}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Even is P is not NP, what about the specific problems on the left side
  of the table? On the basis of what evidence we do believe that these
  particular problems have no efficient algorithms.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Such evidence is provided by \emph{reductions}, which translate one
    search problem into another.
  \item
    What they demonstrate is that they problems on the left side are
    \emph{exactly the same problem}, except that they are stated in
    different languages.
  \end{itemize}
\end{itemize}

\begin{quote}
A search problem is NP-complete if all other search problems reduce to
it.
\end{quote}

\begin{description}
\item[NP]
Class of computational problems for which solutions can be computed by a
non-deterministic Turing machine in polynomial time (or less). Or,
equivalently, those problems for which solutions can be checked in
polynomial time by a deterministic Turing machine.
\item[NP-hard]
Class of problems which are at least as hard as the hardest problems in
NP. Problems in NP-hard do not have to be elements of NP, indeed, they
may not even be decision problems.
\item[NP-complete]
Class of problems which contains the hardest problems in NP. Each
element of NP-complete has to be an element of NP.
\item[NP-easy]
At most as hard as NP, but not necessarily in NP, since they may not be
decision problems.
\item[NP-equivalent]
Exactly as difficult as the hardest problems in NP, but not necessarily
in NP.
\end{description}

\subsubsection{Practice questions}\label{practice-questions-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the minimum cut problem?}

  In graph theory, a minimum cut of a graph is a cut (a partition of the
  vertices of a graph into two disjoint subsets that are joined by at
  least one edge) whose cut set has the smallest number of edges
  (unweighted case) or smallest sum of weights possible. Several
  algorithms exist to find minimum cuts.
\item
  \textbf{Is there a polynomial time algorithm for this problem?}

  Yes -- Krager's algorithm.
\item
  \textbf{Consider the following approach for computing a minimum cut:
  run Kruskal's algorithm on an unweighted graph and remove the last
  edge of the resulting minimum spanning tree to define a cut (randomize
  the selection of the edges among multiple equivalent choices). Show
  that the probability of this cut being the minimum cut is at least
  $\frac{1}{n^2}$}

  So let us see why the cut found in each iteration is the minimum cut
  with probability at least 1/n2. At any stage of Kruskal's algorithm,
  the vertex set V is partitioned into connected components. The only
  edges eligible to be added to the tree have their two endpoints in
  distinct components. The number of edges incident to each component
  must be at least C, the size of the minimum cut in G (since we could
  consider a cut that separated this component from the rest of the
  graph). So if there are k components in the graph, the number of
  eligible edges is at least kC/2 (each of the k components has at least
  C edges leading out of it, and we need to compensate for the
  double-counting of each edge). Since the edges were randomly ordered,
  the chance that the next eligible edge in the list is from the minimum
  cut is at most $C/(kC/2) = 2/k$. Thus, with probability at least
  $1 - 2/k = (k - 2)/k$, the choice leaves the minimum cut intact. But
  now the chance that Kruskal's algorithm leaves the minimum cut intact
  all the way up to the choice of the last spanning tree edge is at
  least

  \[\frac{n - 2}{n} \times \frac{n - 3}{n - 1} \times \frac{n - 4}{n - 2} \times ... \frac{2}{4} \times \frac{1}{4} = \frac{1}{n (n - 1)}\]
\item
  \textbf{What is the running time $n^2$ of the randomized approach that
  computes the minimum cut with high probability through Kruskal's
  algorithm?}

  This means that repeating the process O(n2) times and outputting the
  smallest cut found yields the minimum cut in G with high probability:
  an O(mn2 log n) algorithm for unweighted minimum cuts. Some further
  tuning gives the O(n2 log n) minimum cut algorithm, invented by David
  Karger, which is the fastest known algorithm for this important
  problem.
\item
  \textbf{What is the balanced cut problem?}

  given a graph with $n$ vertices and a budget $b$, partition the
  vertices into two sets $S$ and $T$ such that $|S|,|T| \le n/3$ and
  such that there are at most $b$ edges between $S$ and $T$.
\item
  \textbf{Is there a polynomial time algorithm for this problem?}

  Not unless $P = NP$
\item
  \textbf{What is the knapsack problem?}

  The knapsack problem or rucksack problem is a problem in combinatorial
  optimization: Given a set of items, each with a mass and a value,
  determine the number of each item to include in a collection so that
  the total weight is less than or equal to a given limit and the total
  value is as large as possible. It derives its name from the problem
  faced by someone who is constrained by a fixed-size knapsack and must
  fill it with the most valuable items.
\item
  \textbf{Is there a polynomial time algorithm for this problem?}

  Not unless $P = NP$
\item
  \textbf{What is the subset sum problem and how does it relate to the
  knapsack problem?}

  In computer science, the subset sum problem is an important problem in
  complexity theory and cryptography. The problem is this: given a set
  (or multi-set) of integers, is there a non-empty subset whose sum is
  zero? For example, given the set $\{ -7, -3, -2, 5, 8\}$, the answer
  is yes because the subset $\{ -3, -2, 5 \}$ sums to zero. The problem
  is NP-complete.
\item
  \textbf{What is the class of NP problems?}

  In computational complexity theory, NP is one of the most fundamental
  complexity classes. The abbreviation NP refers to ``nondeterministic
  polynomial time.''

  Intuitively, NP is the set of all decision problems for which the
  instances where the answer is ``yes'' have efficiently verifiable
  proofs of the fact that the answer is indeed ``yes''. More precisely,
  these proofs have to be verifiable in polynomial time by a
  deterministic Turing machine. In an equivalent formal definition, NP
  is the set of decision problems where the ``yes''-instances can be
  accepted in polynomial time by a non-deterministic Turing machine. The
  equivalence of the two definitions follows from the fact that an
  algorithm on such a non-deterministic machine consists of two phases,
  the first of which consists of a guess about the solution, which is
  generated in a non-deterministic way, while the second consists of a
  deterministic algorithm that verifies or rejects the guess as a valid
  solution to the problem.

  The complexity class P is contained in NP, but NP contains many
  important problems, the hardest of which are called NP-complete
  problems, whose solutions are sufficient to deal with any other NP
  problem in polynomial time. The most important open question in
  complexity theory, the P = NP problem, asks whether polynomial time
  algorithms actually exist for NP-complete, and by corollary, all NP
  problems. It is widely believed that this is not the case.
\item
  \textbf{What is the class of P problems?}

  In computational complexity theory, P, also known as PTIME or
  DTIME(nO(1)), is one of the most fundamental complexity classes. It
  contains all decision problems that can be solved by a deterministic
  Turing machine using a polynomial amount of computation time, or
  polynomial time.

  Cobham's thesis holds that P is the class of computational problems
  that are ``efficiently solvable'' or ``tractable''; in practice, some
  problems not known to be in P have practical solutions, and some that
  are in P do not, but this is a useful rule of thumb.
\item
  \textbf{What is the relation between these two classes of problems?}

  If it turned out that P does not equal NP, it would mean that there
  are problems in NP (such as NP-complete problems) that are harder to
  compute than to verify: they could not be solved in polynomial time,
  but the answer could be verified in polynomial time.

  Consider the subset sum problem. Given an arbitrary set, do any of the
  set's subsets' elements sum to zero? If such a solution was given, it
  would be easy to verify. But no known non-polynomial time algorithm
  can solve it, so it's in NP. It's \emph{quickly checkable} but not
  \emph{quickly solvable}.
\item
  \textbf{For instance, is P = NP?}

  It is not known, but the evidence is not suggestive of it.
\item
  \textbf{What does it mean that you can reduce a search problem A to a
  search problem B?}

  Two problems can be reduced to one another if their instances can be
  easily rephrased as instances of the other. For example, the problem
  of solving linear equations in a indeterminate $x$ reduces to the
  problem of solving quadratic equations.

  We say that a language is polynomial time reducible to another
  language if there exists a polynomial time computable function such
  that you can make the elements of one to the other.
\item
  \textbf{What do you need to do in order to provide such a reduction?
  (you can provide a drawing to explain your answer)}
\item
  \textbf{What is the class of NP-complete problems?}

  A decision problem L is NP-complete if it is in the set of NP problems
  and also in the set of NP-hard problems.
\item
  \textbf{What is the class of NP-hard problems?}

  NP-hard problems are problems that are at least as hard as the hardest
  problems in NP.
\item
  \textbf{When is a problem in the class of co-NP problems?}

  We can define the complexity class \textbf{co-NP} as the set of
  languages $L$ such that the inverse of $L$ is in NP.

  The question of whether NP is closed under complement can be rephrased
  as whether NP = co-NP.
\item
  \textbf{Provide an example of a co-NP problem.}

  the subset sum problem: given a finite set of integers, is there a
  non-empty subset that sums to zero? To give a proof of a ``yes''
  instance, one must specify a non-empty subset that does sum to zero.
  The complementary problem is in co-NP and asks: ``given a finite set
  of integers, does every non-empty subset have a non-zero sum?''
\end{enumerate}

\subsection{Examples of reductions between NP complete
problems}\label{examples-of-reductions-between-np-complete-problems}

\subsubsection{Practice questions}\label{practice-questions-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Show that the general satisfiability (SAT) problem reduces to
  the 3-SAT problem.}

  The trick is as follows. Expand the ellipses in the following
  formulation as ``$y_1, y_2, y_3 ...$''

  \[(a_1 \lor a_2 \lor ... \lor a_k)\]

  Is equivalent to the following 3SAT formulation:

  \[(a_1 \lor a_2 \lor y_1)(\lnot y_1 \lor a_3 \lor y_2)(y_{k - 3} \lor a_{k - 1} \lor a_k)\]

  Suppose all the clauses on the right are satisfied. Then at least on
  of the literals $a_1$ through $a_k$ must be true, which would in turn
  force $y_2$ to be true, and so on, eventually falsifying the last
  clause.

  Conversly, if $(a_1 \lor a_2 \lor ... \lor a_k)$ is satisfied, then
  some $a_1$ must be true. Set $y_1, \cdots, y_{i - 2}$ to true and the
  rest to false. This ensures that the clauses on the right are
  satisfied.
\item
  \textbf{Show that the 3-SAT problem reduces to the Independent Set
  problem.}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Graph $G$ has a triangle for each clause with vertices labelled by
    the clauses' literal,s, and has edges between opposite literals.
  \item
    The goal $g$ is set to the number of clauses.
  \end{itemize}
\item
  \textbf{Show that the Independent Set problem reduces to the Vertex
  cover problem.}

  To reduce independent set to vertex cover we just need to notice that
  a set of nodes $S$ is a vertex confer of graph $G = VE$ if and only if
  the remaining nodes, $V - S$, are an independent set of $G$.

  Therefore, to solve an instance $(G, g)$ of independent set, simply
  look for a vertex cover of $G$ with $|V| - g$ nodes. If such a vertex
  cover exists, then take all the nodes NOT in it. If no such vertex
  cover exists, then $G$ cannot possibly have an indepandt set of size
  $g$.
\item
  \textbf{Show that the Independent Set problem reduces to the Clique
  problem.}

  Define the complement of a graph $G = (V, E)$ to be
  $\bar{G} = (V, \bar{E})$, where $\bar{E}$ contains precisely those
  unordered pairs of vertices that are not in $E$. Then, a set of nodes
  $S$ is an independent set of $G$ if and only if $S$ is a clique of
  $\bar{G}$. TO paraphrase, these nodes have no edges between them if
  and only if they have all possible edges between them in $\bar{G}$
\item
  \textbf{Show that the Circuit SAT problem reduces to the SAT problem.}

  Notice that SAT asks for a satisfying truth assignment of a circuit
  that has this simple structure: a bunch of AND gates at the top join
  the clauses, and the result of this big AND is the oupt. Each clause
  is the OR of the literals.
\item
  \textbf{What is the importance of this reduction?}

  Naturally, for any input length (number of input bits) the circuit
  will be scaled to the appropriate number of inputs, but the total
  number of gates of the circuit will be polynomial in the number of
  inputs. If the polynomial algorithm in question solves a problem that
  requires a yes or no answer (as is the situation with C: ``Does S
  encode a solution to the instance encoded by I?''), then this answer
  is given at the output gate.

  We conclude that, given any instance I of problem A, we can construct
  in polynomial time a circuit whose known inputs are the bits of I, and
  whose unknown inputs are the bits of S, such that the output is true
  if and only if the unknown inputs spell a solution S of I . In other
  words, the satisfying truth assignments to the unknown inputs of the
  circuit are in one-to-one correspondence with the solutions of
  instance I of A. The reduction is complete.
\end{enumerate}

\subsection{Intelligent Exhaustive Search, Intro to Approximation
Algorithms}\label{intelligent-exhaustive-search-intro-to-approximation-algorithms}

\subsubsection{Practice questions}\label{practice-questions-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the idea behind backtracking search in order to solve
  NP-complete problems?}

  Backtracking is based on the observation that it is often possible to
  reject a solution by looking at just a small portion of it.
\item
  \textbf{Describe a general framework for backtracking search.}

  A backtracking algorithm requires a test that looks at a subproblem
  and quickly declares one of three outcomes:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Failure: the subproblem has no solution.
  \item
    Success: a solution to the subproblem is found.
  \item
    Uncertainty.
  \end{enumerate}
\item
  \textbf{Describe the application of backtracking search to the
  satisfiability problem, i.e., which sub-problems are expanded at each
  iteration and which branching variable is considered?}

  For a Boolean clause $\phi(a, b, ..., c)$, branch on any variable, and
  seek clauses that are violated by assignments of \texttt{true} or
  \texttt{false}. It makes sense to choose the subproblem that contains
  the smallest clause and to then branch on a variable in that clause.
  If this clause happens to be a singleton, then at least one of the
  resulting branches will be terminated.
\item
  \textbf{Describe the general branch-and-bound approach for
  optimization problems.}

  As before, we will deal with partial solutions, each of which
  represents a subproblem, namely, what is the (cost of the) best way to
  complete this solution? And as before, we need a basis for eliminating
  partial solutions, since there is no other source of efficiency in our
  method. To reject a subproblem, we must be certain that its cost
  exceeds that of some other solution we have already encountered. But
  its exact cost is unknown to us and is generally not efficiently
  computable. So instead we use a quick lower bound on this cost.
\item
  \textbf{Describe an application of branch-and-bound to the traveling
  salesman problem.}
\item
  \textbf{How is the approximation ratio of an approximation algorithm
  computed?}

  Suppose now that we have an algorithm $A$ for our problem which, given
  an instance $I$, returns a solution with value $A(I)$. The
  \emph{approximation ratio} of algorithm $A$ is defined to be:

  \[\alpha_{A} = \text{max}_{i} \frac{A(I)}{\text{OPT}(I)}\]

  In other words, $\alpha_{A}$ measures by the factor by which the
  output of the algorithm $A$ exceeds the optimal solution, on the
  worst-case input. The approximation ration can also be defined for
  maximization problems, such as independent set, in the same way --
  except that to get a number larger than 1 we take the reciprocal.
\item
  \textbf{Provide an approximation algorithm for the vertex cover
  problem. What approximation ratio does it achieve?}
\end{enumerate}

\subsection{Approximation Algorithms, Local Search
Heuristics}\label{approximation-algorithms-local-search-heuristics}

\subsubsection{Chapters 9.2 from DPV: Approximation
algorithms}\label{chapters-9.2-from-dpv-approximation-algorithms}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  For every instance $I$, let us denote $\text{OPT}(I)$ the value
  (benefit or cost) of the optimum solution. It makes the math simpler
  to \emph{assume that this is always a positive energy}.
\item
  For any instance $I$ of size $n$, we showed that this greedy algorithm
  is guaranteed to quickly find a set cover of cardinality of at most
  $\text{OPT}(I)\log n$.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    This factor is known as the approximation guarantee of the
    algorithm.
  \end{itemize}
\item
  More generally, consider any minimization problem. Suppose now that we
  have an algorithm $A$ for our problem which, given instance $I$,
  returns a solution with value $A(I)$. This \emph{approximation ratio}
  of algorithm $A$ is defined to be:
\end{itemize}

\[\alpha_{A} = \text{max}_{i} \frac{A(I)}{\text{OPT}(I)}\]

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  In other words, $\alpha_A$ measures by the factor by which the output
  of algorithm $A$ exceeds the optimal solution, on the worst-case
  input.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The approximation ratio can also be defined for maximization
    problems, such as independent set, in the same way -- except that to
    a number larger than one we take the reciprocal.
  \end{itemize}
\end{itemize}

\paragraph{Clustering}\label{clustering}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  We have some data that we want to divide into groups.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    It is useful to define ``distances'' between these data points --
    numbers that capture how close or far they are from one another.
  \item
    Assume that we have such distances and that they satisfy the usual
    \emph{metric} properties:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      $\forall x, y d(x, y) \ge 0 $
    \item
      $d(x, y) = 0$ if an only if $x = y$.
    \item
      $d(x, y) = d(y, x)$
    \item
      (Triangle inequality) $d(x, y) \le d(x, z) + d(z, y)$
    \end{enumerate}
  \end{itemize}
\item
  We would like to partition the data points into groups that are
  compact in the sense of having small diameter.
\end{itemize}

\begin{quote}
k-CLUSTER

\emph{Input}: Points $X = \{ x_1, \cdots, x_n \}$ with underlying metric
$d(\cdot, \cdot)$; integer $k$

\emph{Output}: A partition of the points into $k$ clusters
$C_1, \cdots C_k$.

\emph{Goal}: Minimize the diameter of the clusters,

$\text{max}_j \text{max}_{x_a, x_b \in C_j} d(x_a, x_b)$
\end{quote}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  One way to visualize this task is to imagine $n$ points in space,
  which are to be convered by $k$ spheres of equal size. What is the
  smallest possible diameter of the sphere?
\end{itemize}

\paragraph{TSP}\label{tsp}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  if the distances between cities satisfy the metric properties, then
  there is an algorithm that outputs a tour of length at most $1.5$
  times optimal
\end{itemize}

\[\text{TSP cost} \le \text{cost of this path} \le \text{MST cost}\]

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  We somehow need to use the MST to build a traveling salesman tour. If
  we can use each edge \emph{twice}, then by following the shape of the
  MST, we end up with a tour that visits all the cities, some of them
  more than once.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Therefore, this tour has a length of at most twice the MST cost,
    which as we've already seen is at most twice the TSP cost.
  \end{itemize}
\item
  This is the result we wanted, but we aren't quite done because our
  tour visits come cities multiple times and is therefore not legal.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    To fix the problem, the tour should simply skip any city it is about
    to revisit, and instead move directly the \emph{new} city in its
    list.
  \end{itemize}
\end{itemize}

\subsubsection{Chapter 9.3 from DPV: Local search
heuristics}\label{chapter-9.3-from-dpv-local-search-heuristics}

\paragraph{Graph partitioning}\label{graph-partitioning}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The problem of graph partitioning arises in a diversity of
  application.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    We saw a special case in Balanced Cut.
  \end{itemize}
\end{itemize}

\begin{quote}
\textbf{Graph partitioning}

\emph{Input}: An undirected graph $G = (V, E)$ with nonnegative edge
weights; a real number $\alpha \in (o, 1/2]$

\emph{Output}: A partition of the vertices into two groups, $A$ and $B$,
each of size at least $\alpha |V|$.

\emph{Goal}: Minimize the capacity of the cut $(A, B)$
\end{quote}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  This variant is NP hadd.
\item
  Focus on the special case $\alpha = 1/2$, in which $A$ and $B$ are
  forced to contain exactly half the vertices. The apparent loss of
  generality is purely cosmetic, as Graph partitioning reduces to this
  particular case.
\item
  We need to decide upon a neighborhood structure for our problem, and
  there is one one obvious way.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Let $(A, B)$ with $|A| = |B|$ be a candidate solution; we will
    define its neighbors to be all solutions obtainable by swapping one
    pair of vertices across the cut, that is, all solution of the form
    $(A - \{a\} + \{b\}, B - \{b\} + \{a\})$ where $a \in A$ and
    $b \in B$.
  \end{itemize}
\item
  There is still a lot of room for improve in terms of the quality of
  the solution produced. The search space includes some local optima
  that are quite far from the global solution.
\end{itemize}

\subsubsection{Practice questions}\label{practice-questions-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{When does a distance function satisfy metric properties?}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    $\forall x, y d(x, y) \ge 0 $
  \item
    $d(x, y) = 0$ if an only if $x = y$.
  \item
    $d(x, y) = d(y, x)$
  \item
    (Triangle inequality) $d(x, y) \le d(x, z) + d(z, y)$
  \end{enumerate}
\item
  \textbf{What is the k-clustering NP-complete problem?}

  k-means clustering aims to partition n observations into k clusters in
  which each observation belongs to the cluster with the nearest mean,
  serving as a prototype of the cluster.
\item
  \textbf{Provide a simple approximation scheme for the k-clustering
  problem.}

  The idea is to pick k of the data points as cluster centers and to
  then assign each of the remaining points to the center closest to it,
  thus creating k clusters. The centers are picked one at a time, using
  an intuitive rule: always pick the next center to be as far as
  possible from the centers chosen so far

\begin{verbatim}
Pick any point u_1 in X as the first cluster center
for i = 2 to k:
    let u_i be the point in X that is farthest from u_1, ..., u_{i - 1}
create k clusters: C_i = {all x in X whose closest center is in u_i}
\end{verbatim}
\item
  \textbf{What is the approximation ratio that it achieves?}

  the resulting diameter is guaranteed to be at most twice optimal.
\item
  \textbf{Prove it.}

  Let $x \in X$ be the point farthest from $u_1, ..., u_k$ (in other
  words the next center we could have chose, if we wanted $k+1$ of
  them), and let $r$ be its distance to its closest center. Then every
  point in $X$ must be within distance $r$ of its cluster center. By the
  triangle inequality, this means that every cluster has diameter at
  most $2r$.
\item
  \textbf{Provide an approximation algorithm for the traveling salesman
  problem given that the underlying graph has edge weights that satisfy
  metric properties.}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Construct a minimum spanning tree T for G.
  \item
    Duplicate all edges of T. That is, wherever there is an edge from u
    to v, add a second edge from v to u. This gives us an Eulerian graph
    H.
  \item
    Find an Eulerian circuit C in H. Clearly, its span is twice the span
    of the tree.
  \item
    Convert the Eulerian circuit C of H into a Hamiltonian cycle of G in
    the following way: walk along C, and each time you are about to come
    into an already visited vertex, skip it and try to go to the next
    one (along C).
  \end{enumerate}
\item
  \textbf{What is the approximation ratio for this algorithm? Prove it.}

  if the distances between cities satisfy the metric properties, then
  there is an algorithm that outputs a tour of length at most 1.5 times
  optimal.
\item
  \textbf{Describe the general local search framework.}

  By its incremental process of introducing small mutations, trying them
  out, and keeping them if they work well. This paradigm is called
  \emph{local search} and can be applied to any optimization task.

\begin{verbatim}
let s be any initial solution
while there is some solution s' in the neighborhood of s
    for which cost(s') < cost(s): replace s by s'
return s
\end{verbatim}

  On each iteration, the current solution is replaced by a better one
  close to it, in its \emph{neighborhood}. This neighborhood structure
  is something we impose upon the problem and is the central designing
  decision in local search.
\item
  \textbf{Provide a local search approach for the traveling salesman
  problem.}

  Assume we have all inter point distances between $n$ cities, giving a
  search space of $(n - 1)!$ different tours. What is a good notion of
  neighborhood?

  The most obvious notion is to consider two tours as being close if
  they different in just a few edges, but they cannot differ in just
  one. We define the \emph{2-change} neighbor of tour $s$ as being the
  set of tours that can be obtained by removing two edges of $s$ and
  then putting in two other edges.

  We now have a well-defined local search procedure. How does it measure
  up under our two standard criteria for algorithms -- what is its
  overall running time and does it always return the best solution?

  Neither of those questions have a satisfactory answer. Each iteration
  is fast, because a tour has only $O(n^2)$ neighbors. However, it is
  not clear how many iterations will be needed: perhaps there might be
  an exponential number. Likewise, all we can easily say about the final
  tour is that it is \emph{locally optimal} -- that is, it is superior
  to the tours in its immediate neighborhood. There might be better
  solutions further away.

  To combat this, 3-change, 4-change \ldots{}
\item
  \textbf{Provide a local search strategy for the graph partitioning
  problem.}

  Let $(A, B)$ with $|A| = |B|$ be a candidate solution; we will define
  its neighbors to be all solutions obtainable by swapping one pair of
  vertices across the cut, that is, all solution of the form
  $(A - \{a\} + \{b\}, B - \{b\} + \{a\})$ where $a \in A$ and
  $b \in B$.
\item
  \textbf{How can you deal with local optimal in the context of local
  search?}

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Randomization and restarts
  \item
    Simulated annealing
  \end{itemize}
\end{enumerate}

\bibliography{}


\end{document}